{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate Sustainability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [0 - Imports and Dependencies](#imports-and-dependencies)\n",
    "- [1 - Data Loading](#data-loading)\n",
    "- [2 - Exploratory Data Analysis](#explatory-data-analysis)\n",
    "- [3 - Training, testing and evaluating ML models](#training-testing-models)\n",
    "- [4 - Classifiers](#classifiers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Imports and Dependencies<a id=\"imports-and-dependencies\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, please make sure you have the following packages installed:\n",
    "\n",
    "- `pandas`: For handling data in dataframes.\n",
    "- `datasets`: For loading datasets.\n",
    "- `spacy`: For natural language processing.\n",
    "- `scikit-learn`: For machine learning algorithms, vectorization, model evaluation, and pipelines.\n",
    "- `wordcloud`: For creating a visual interpretation of text analytics.\n",
    "- `nlpaug`: For data augmenting\n",
    "\n",
    "You can install these packages using the following command:\n",
    "\n",
    "```bash\n",
    "pip install pandas datasets spacy scikit-learn wordcloud nlpaug\n",
    "\n",
    "```\n",
    "or using conda:\n",
    "```bash\n",
    "conda install pandas datasets spacy scikit-learn wordcloud nlpaug\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Splitting imports into three cells to avoid errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing libraries for data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.word as naw \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting libraries for data preprocessing and modeling\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             HashingVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             confusion_matrix, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV,\n",
    "                                     train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Loading<a id=\"data-loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset('climatebert/environmental_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display to show how the format of the dataset looks like\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each split\n",
    "\n",
    "env_claim_train = pd.DataFrame(dataset['train'])\n",
    "env_claim_test = pd.DataFrame(dataset['test'])\n",
    "env_claim_val = pd.DataFrame(dataset['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color codes\n",
    "class Colors:\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "# Displaying the characteristics of the data\n",
    "print(Colors.OKGREEN + \"Training data\" + Colors.ENDC)\n",
    "print(f\"The shape of the training data is: {env_claim_train.shape}\")\n",
    "display(env_claim_train.head())\n",
    "display(env_claim_train.label.value_counts())\n",
    "\n",
    "print(Colors.OKBLUE + \"\\nTest data\" + Colors.ENDC)\n",
    "print(f\"The shape of the test data is: {env_claim_test.shape}\")\n",
    "display(env_claim_test.head())\n",
    "display(env_claim_test.label.value_counts())\n",
    "\n",
    "print(Colors.OKCYAN + \"\\nValidation data\" + Colors.ENDC)\n",
    "print(f\"The shape of the validation data is: {env_claim_val.shape}\")\n",
    "display(env_claim_val.head())\n",
    "display(env_claim_val.label.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis<a id=\"#explatory-data-analysis\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets \n",
    "claim_dataset = pd.concat([env_claim_train, env_claim_test, env_claim_val], ignore_index = True)\n",
    "print(\"Number of claims in the dataset:\", claim_dataset.shape[0])    # observations\n",
    "print(\"Number of variables in the dataset:\", claim_dataset.shape[1]) # variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs \n",
    "print(\"Number of NaNs:\")\n",
    "display(claim_dataset.isna().sum())\n",
    "\n",
    "# Duplicates\n",
    "print(\"Number of duplicates:\")\n",
    "display(claim_dataset.duplicated().sum())\n",
    "\n",
    "# Variable types\n",
    "print(\"Variable types:\")\n",
    "claim_dataset.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count by Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that show the avergage number of words in each claim and a graphical representation by class\n",
    "\n",
    "def word_count_graph(claim_dataset):\n",
    "\t# Word count\n",
    "\tclaim_dataset[\"word count\"] = claim_dataset[\"text\"].apply(lambda x: len(x.split()))\n",
    "\tprint(\"The average number of words in each claim is equal to:\", round(claim_dataset[\"word count\"].mean(),0), \"words.\")\n",
    "\n",
    "\t# Graphical representation by class\n",
    "\tclass_1_counts = claim_dataset[claim_dataset[\"label\"] == 1][\"word count\"]\n",
    "\tclass_2_counts = claim_dataset[claim_dataset[\"label\"] == 0][\"word count\"]\n",
    "\n",
    "\tplt.hist(class_1_counts, bins = range(11, 39), alpha = 0.5, label = \"Environmental Claim\", color = \"#4958B5\")\n",
    "\tplt.hist(class_2_counts, bins = range(11, 39), alpha = 0.5, label = \"Non-environmental Claim\", color = \"#8DB8B7\")\n",
    "\tplt.xlabel(\"Word count\")\n",
    "\tplt.ylabel(\"Frequency\")\n",
    "\tplt.title(\"Number of Words in Each Claim by Class\")\n",
    "\tplt.legend(loc = \"upper right\")\n",
    "\tplt.show()\n",
    "\n",
    "# Applying the function\n",
    "word_count_graph(claim_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the Spacy sp function to each row of the 'text' column\n",
    "claim_dataset[\"spacy object\"] = claim_dataset[\"text\"].apply(sp)\n",
    "\n",
    "# Filter stopwords, punctuation and spaces\n",
    "def filter_tokens(token):\n",
    "    return not token.is_stop and not token.is_punct and not token.is_space\n",
    "\n",
    "# Remove stopwords, punctuation, and whitespace from each Spacy object\n",
    "claim_dataset[\"filtered tokens\"] = claim_dataset[\"spacy object\"].apply(lambda doc: [token.text for token in doc if filter_tokens(token)])\n",
    "\n",
    "print(\"This is the first sentence before filtering:\", claim_dataset.iloc[0,0])\n",
    "print(\"\\nThis is the first sentence after filtering:\", claim_dataset.iloc[0,4])\n",
    "\n",
    "# Calculating new average value of words per claim\n",
    "number_words = [len(x) for x in claim_dataset[\"filtered tokens\"]]\n",
    "print(\"\\nThe average number of words per claim is now:\", round(np.mean(number_words),0))\n",
    "display(claim_dataset.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Claims versus Non-Environmental Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean \n",
    "print(\"Average number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").mean().round())\n",
    "\n",
    "# Median\n",
    "print(\"\\nMedian number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud hue by class label\n",
    "\n",
    "# Join the strings in each list into a single string\n",
    "claim_dataset[\"joined tokens\"] = claim_dataset[\"filtered tokens\"].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# For Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==1][\"joined tokens\"])\n",
    "wordcloud = WordCloud( background_color = \"white\", colormap = \"Greens\").generate(text)\n",
    "\n",
    "ax1.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "ax1.set(title = \"WordCloud of Environmental Claims\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# For Non-Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==0][\"joined tokens\"])\n",
    "wordcloud = WordCloud(background_color = \"white\", colormap = \"Reds\").generate(text)\n",
    "\n",
    "ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "ax2.set(title = \"WordCloud of Non-Environmental Claims\")\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the top most frequent words\n",
    "def top_words(claim_dataset, label, n=10):\n",
    "    top = Counter([item for sublist in claim_dataset[\"joined tokens\"]   # sublist is a list of words in each claim\n",
    "                  [claim_dataset[\"label\"] == label] for item in str(sublist).split()])\n",
    "    temp = pd.DataFrame(top.most_common(n))     # Create a dataframe with the top n words\n",
    "    temp.columns = [\"Common Words\", \"Count\"]    # Naming the columns\n",
    "    temp.index = np.arange(1, len(temp) + 1)    # Setting first index to 1\n",
    "    return temp\n",
    "\n",
    "\n",
    "# Top 10 most frequent words for environmental claims\n",
    "print(\"Top 10 most frequent words for environmental claims:\")\n",
    "env_top_words = top_words(claim_dataset, 1)\n",
    "display(env_top_words.style.background_gradient(cmap=\"Greens\"))\n",
    "\n",
    "# Top 10 most frequent words for non-environmental claims\n",
    "print(\"\\nTop 10 most frequent words for non-environmental claims:\")\n",
    "nonenv_top_words = top_words(claim_dataset, 0)\n",
    "display(nonenv_top_words.style.background_gradient(cmap=\"Reds\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Populating with more environmental claims\n",
    "\n",
    "Since are data is heavily underrepresented in class 1, we used ChatGPT 3 and 4 to generate different environmental claims. These can be found in the `env_claims_gpt3.json` and `env_claims_gpt4.json` in the `./data` folder.\n",
    "\n",
    "We will append these statements to the ones we already have, and later investigate whether these can improve our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the env claims from env_claims_gpt4.json\n",
    "\n",
    "claims_gpt4 = []\n",
    "# Load the env_claims_gpt3.json file\n",
    "with open('./data/env_claims_gpt4.json') as json_file:\n",
    "\tdata = json.load(json_file)\n",
    "\n",
    "\tfor i in range(len(data['claims'])):\n",
    "\t\tclaims_gpt4.append(data['claims'][i]['claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_gpt3 = []\n",
    "# Load the env_claims_gpt3.json file\n",
    "with open('./data/env_claims_gpt3.json') as json_file:\n",
    "\tdata = json.load(json_file)\n",
    "\n",
    "\tfor i in range(len(data['claims'])):\n",
    "\t\tclaims_gpt3.append(data['claims'][i]['claim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of claims\n",
    "print(\"Number of claims in the dataset (GPT4):\", len(claims_gpt4))    # observations\n",
    "print(\"Number of claims in the dataset (GPT3):\", len(claims_gpt3))    # observations\n",
    "print(\"Total number of claims in the dataset:\", len(claims_gpt4)+len(claims_gpt3))    # observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list to a dataframe\n",
    "\n",
    "claims_gpt4_df = pd.DataFrame(claims_gpt4, columns = ['text'])\n",
    "claims_gpt3_df = pd.DataFrame(claims_gpt3, columns = ['text'])\n",
    "\n",
    "# Adding the label column\n",
    "claims_gpt4_df['label'] = 1\n",
    "claims_gpt3_df['label'] = 1\n",
    "\n",
    "# Concatenating the two dataframes\n",
    "claims_gpt_df = pd.concat([claims_gpt4_df, claims_gpt3_df], ignore_index=True)\n",
    "display(claims_gpt_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we concat the generated data, we can see how the distribution and inbalance has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new df with the original claims and the generated claims\n",
    "claim_pop = pd.concat([claim_dataset, claims_gpt_df], ignore_index=True)\n",
    "\n",
    "# Displaying how the generated has changed the distribution of the dataset\n",
    "print(\"Distribution of the dataset before adding the generated claims:\")\n",
    "display(claim_dataset['label'].value_counts(normalize=True).round(2))\n",
    "\n",
    "print(\"\\nDistribution of the dataset after adding the generated claims:\")\n",
    "display(claim_pop['label'].value_counts(normalize=True).round(2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Energy Claims using N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Still need to finish this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample of claims with word \"energy\" inside\n",
    "energy_df = claim_dataset[claim_dataset[\"joined tokens\"].str.contains(\"energy\")]\n",
    "print(\"In the original dataset, there are\", len(energy_df), \"claims containing the word 'energy'.\")\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 1\n",
    "energy_df_1 = energy_df[energy_df[\"label\"] == 1]\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 0\n",
    "energy_df_0 = energy_df[energy_df[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculating most frequent N-Grams given corpus, n-grams\n",
    "\n",
    "def top_n_ngram(energy_corpus, ngram = 3):\n",
    "    vec = CountVectorizer(ngram_range = (ngram,ngram)).fit(energy_corpus)\n",
    "    words_bag = vec.transform(energy_corpus)  # Have the count of  all the words for each claim\n",
    "    sum_words = words_bag.sum(axis = 0)       # Calculates the count of all the word in the whole claim\n",
    "    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n",
    "    return words_freq\n",
    "\n",
    "# Call function on both datasets \n",
    "pop_words_1 = top_n_ngram(energy_df_1[\"joined tokens\"], 3)  \n",
    "pop_words_0 = top_n_ngram(energy_df_0[\"joined tokens\"], 3)  \n",
    "\n",
    "# Select top 20 N-Grams having 'energy' in text \n",
    "pop_energy_1 = [t for t in pop_words_1 if \"energy\" in t[0]]\n",
    "pop_energy_1 = pop_energy_1[:20]\n",
    "pop_energy_0 = [t for t in pop_words_0 if \"energy\" in t[0]]\n",
    "pop_energy_0 = pop_energy_0[:20]\n",
    "\n",
    "# Graphical representation\n",
    "\n",
    "# Extract x and y values from each list\n",
    "x1, y1 = zip(*pop_energy_1)\n",
    "x2, y2 = zip(*pop_energy_0)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 2, figsize = (20,9))\n",
    "fig.subplots_adjust(hspace = 1.2)\n",
    "\n",
    "# Create the first bar plot on ax1\n",
    "ax1.bar(x1, y1, color = \"#4958B5\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xticks(range(len(x1)))\n",
    "ax1.set_xticklabels(x1, rotation = 90)\n",
    "ax1.set_title(\"Top 20 Energy 3-Grams in Environmental Claims\")\n",
    "\n",
    "# Create the second bar plot on ax2\n",
    "ax2.bar(x2, y2, color = \"#8DB8B7\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xticks(range(len(x2)))\n",
    "ax2.set_xticklabels(x2, rotation = 90)\n",
    "ax2.set_title(\"Top 20 Energy 3-Grams in Non-Environmental Claims\")\n",
    "\n",
    "# Add x-axis label\n",
    "fig.add_subplot(111, frameon = False)\n",
    "plt.tick_params(labelcolor = \"none\", top = False, bottom = False, left = False, right = False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the 3-gram lists, it becomes apparent that the terms in Class 1 are more focused on energy generation, efficiency, and reduction of consumption with a clear emphasis on renewable and clean technology. Key phrases like 'improve energy efficiency', 'renewable energy projects', and 'reduce energy consumption' suggest that Class 1 is associated with environmentally proactive actions or strategies.\n",
    "\n",
    "On the other hand, Class 0 appears to be more concerned with the management and infrastructure of energy, including the use of renewable sources, but with notable mentions of 'energy management systems', 'incineration energy recovery', and 'energy storage capacity'. This class seems to focus more on the operational aspects and physical assets related to energy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the train and test datasets for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = env_claim_train['text'], env_claim_train['label']\n",
    "X_test, y_test = env_claim_test['text'], env_claim_test['label']\n",
    "X_val, y_val = env_claim_val['text'], env_claim_test['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Labelled Data and Defining Base Rate\n",
    "\n",
    "TODO: Include gpt generated claims and show graph for both dataframes and label categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class:\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "\n",
    "\n",
    "# Test Set\n",
    "print(Colors.OKBLUE + \"\\nTest set per class:\" + Colors.ENDC)\n",
    "display(y_test.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class:\" + Colors.ENDC)\n",
    "display(y_val.value_counts())\n",
    "\n",
    "\n",
    "# Creating a function that takes in y's returns a a plot of the distribution of the classes\n",
    "def plot_class_distribution(y_list: list):\n",
    "    ''' \n",
    "    Function that plots the class distribution\n",
    "    Input: List of y vars\n",
    "    Output: Plot of distribution\n",
    "    '''\n",
    "    is_list = isinstance(y_list, list)\n",
    "    if is_list:\n",
    "        outcome_variable = pd.concat(y_list)\n",
    "\n",
    "    else:\n",
    "        outcome_variable = y_list\n",
    "    outcome_variable.value_counts().plot.bar(\n",
    "        color=[\"#4958B5\", \"#8DB8B7\"], grid=False)\n",
    "    plt.ylabel(\"Number of observations\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.title(\"Number of Observations per Class\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_class_distribution(claim_dataset['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating base rate\n",
    "\n",
    "outcome_variable = claim_dataset['label']\n",
    "base_rate = round(len(outcome_variable[outcome_variable == 0]) / len (outcome_variable), 4)\n",
    "print(f'The base rate is: {base_rate*100:0.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the graph above with the populated data from ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting class distribution for the merged populated and given data\n",
    "\n",
    "plot_class_distribution(claim_pop['label'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Labelled Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Comment: I'm not sure if we need to balance the test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of \"0\" outcomes in the training set\n",
    "train_zeros_idx = pd.Series(y_train[y_train == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "train_zeros_sample_idx = train_zeros_idx.sample(n = sum(y_train == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced training set\n",
    "X_train_bal = pd.concat([X_train[y_train == 1], X_train[train_zeros_sample_idx]])\n",
    "y_train_bal = pd.concat([y_train[y_train == 1], y_train[train_zeros_sample_idx]])\n",
    "\n",
    "\n",
    "# Get indices of \"0\" outcomes in the test set\n",
    "test_zeros_idx = pd.Series(y_test[y_test == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "test_zeros_sample_idx = test_zeros_idx.sample(n = sum(y_test == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced test set\n",
    "X_test_bal = pd.concat([X_test[y_test == 1], X_test[test_zeros_sample_idx]])\n",
    "y_test_bal = pd.concat([y_test[y_test == 1], y_test[test_zeros_sample_idx]])\n",
    "\n",
    "# Get indices of \"0\" outcomes in the validation set\n",
    "val_zeros_idx = pd.Series(y_val[y_val == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "val_zeros_sample_idx = val_zeros_idx.sample(n = sum(y_val == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced validation set\n",
    "X_val_bal = pd.concat([X_val[y_val == 1], X_val[val_zeros_sample_idx]])\n",
    "y_val_bal = pd.concat([y_val[y_val == 1], y_val[val_zeros_sample_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations per class after balancing the classes:\\n\")\n",
    "\n",
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class\" + Colors.ENDC)\n",
    "display(y_train_bal.value_counts())\n",
    "      \n",
    "\n",
    "# Test Set \n",
    "print(Colors.OKBLUE + \"\\nTest set per class\" + Colors.ENDC)\n",
    "display(y_test_bal.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class\" + Colors.ENDC)\n",
    "display(y_val_bal.value_counts())\n",
    "\n",
    "print(\"\\nThe new balanced dataset contains\", len(y_train_bal + y_test_bal + y_val_bal) , \"observations.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training, testing and evaluating ML models<a id=\"training-testing-models\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy\n",
    "\n",
    "We will use the following strategy to train, test and evaluate our models:\n",
    "1. Define different tokenization functions\n",
    "   1. Test different tokenization functions on the Logistic Regression model\n",
    "2. Define different vectorization functions\n",
    "   1. Test different vectorization functions on the Logistic Regression model\n",
    "3. Fine-tune hyperparameters\n",
    "4. Compare the performance of the different pipelines\n",
    "\n",
    "We will in the Classifiers section define different models and compare the performance with the Logistic Regression Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression with different tokenization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating several helper functions\n",
    "\n",
    "# Create a spaCy tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def simple_spacy_tokenizer(text):\n",
    "    return [tok.lemma_.lower() for tok in nlp(text) if not tok.is_stop and tok.is_alpha]\n",
    "\n",
    "def spacy_tokenizer_ngrams(text):\n",
    "    # Parse the text with spaCy's language model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Generate n-grams\n",
    "    def generate_ngrams(doc, n):\n",
    "        return [' '.join(doc[i:i+n]) for i in range(len(doc) - n + 1)]\n",
    "\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # Remove stop words and non-alphabetical tokens\n",
    "        if tok.is_alpha and not tok.is_stop:\n",
    "            # Lemmatize and lower case the token\n",
    "            tokens.append(tok.lemma_.lower().strip())\n",
    "\n",
    "        # If the token is a named entity, add it to the list\n",
    "        if tok.ent_type_:\n",
    "            tokens.append(tok.text)\n",
    "\n",
    "    # Add bi-grams to the list of tokens\n",
    "    tokens.extend(generate_ngrams(tokens, 2))\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `simple_spacy_tokenizer` tokenizes the input text, i.e. For each token, it checks if it is an alphabetical character and if it is not a stop word (commonly used words like 'is', 'the', 'and', etc., that do not carry significant meaning on their own). If the token meets these criteria, it is lemmatized, which means it is converted to its base or dictionary form (for example, 'running' becomes 'run'). The function then converts the token to lowercase and strips any leading or trailing white space. The functon `spacey_tokenizer_ngrams` includes bigrams, the function gives a machine learning model a better chance of understanding the text accurately. The final output is a list of processed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_report_prettify(report):\n",
    "    ''' \n",
    "    Creates a df from the precision_recall_fscore_support function\n",
    "    Input: model performance metrics\n",
    "    '''\n",
    "    out_df = pd.DataFrame(report).transpose()\n",
    "    out_df.columns = ['precision', 'recall', 'f1-score', 'support']\n",
    "    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n",
    "    avg_tot.index = [\"avg/total\"]\n",
    "    out_df = pd.concat([out_df, avg_tot])\n",
    "    out_df['support'] = out_df['support'].apply(lambda x: int(x))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_score):\n",
    "    '''\n",
    "    This function plots the ROC curve.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label=\"Random Classifier\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=['0', '1']):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    '''\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=\"Blues\", cbar=False)\n",
    "    \n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [None, 0, None]\n",
    "\n",
    "def evaluate_model(vectorizer, classifier, X_train, y_train, X_test, y_test, with_confusion_matrix=False):\n",
    "    ''' \n",
    "    Function to evaluate the model performance\n",
    "    Input: vectorizer, classifier, X_train, y_train, X_test, y_test\n",
    "    Output: predicted y and y score\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "\n",
    "    # Train the model\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_score = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Adding a title to the dataframe based on the model and vectorizer used and tokenizer\n",
    "    title = str(classifier).split('(')[0] + ' with ' + str(vectorizer).split('(')[0]\n",
    "    print(\"-\" * len(title))\n",
    "    print(Colors.OKGREEN + title + Colors.ENDC)\n",
    "    print(\"-\" * len(title))\n",
    "\n",
    "    # Calculate accuracy and print the performance metrics\n",
    "    performance_df = classification_report_prettify(precision_recall_fscore_support(y_test, y_pred))\n",
    "    display(performance_df)\n",
    "    accuracy = accuracy_score(y_test, y_pred)   # Accuracy is the number of correct predictions divided by the total number of predictions\n",
    "    print(Colors.OKBLUE + f'Accuracy: {accuracy*100:0.2f}%' + Colors.ENDC)    # Print the accuracy with 3 decimal points\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if with_confusion_matrix:\n",
    "        plot_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    #Updating the best model\n",
    "    global best_model\n",
    "    if accuracy > best_model[1]:\n",
    "        best_model = [pipe, accuracy, performance_df]\n",
    "        print(Colors.OKGREEN + \"\\nBest model updated!\" + Colors.ENDC)\n",
    "    else:\n",
    "        print(Colors.OKBLUE + \"\\nBest model not updated!\" + Colors.ENDC)\n",
    "        \n",
    "    return y_pred, y_score, performance_df # Return the predicted y's and y score and the performance dataframe\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call create a vectorizer and a logistic regression model and evaluate it on the functions created above with the two different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc_curves(names, y_scores, title=\"\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot the ROC curve for each vectorizer/tokenizer/model combination\n",
    "    for name, y_score in zip(names, y_scores):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=name+(' (area = %0.3f)' % roc_auc))\n",
    "\n",
    "    # Plot the ROC curve of a purely random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "    # Plot the ROC curve of a perfect classifier\n",
    "    plt.plot([0, 0, 1], [0, 1, 1], 'k:', label='Perfect Classifier')\n",
    "\n",
    "    # Add labels and legend to the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves {title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "tokenizer_list = [simple_spacy_tokenizer,\n",
    "                  spacy_tokenizer_ngrams]   # Tokenizer list\n",
    "\n",
    "tokenizer_y_score = []  # List to store the y_score for each tokenizer\n",
    "\n",
    "# Going through list of tokenizers and evaluating the model\n",
    "for tokenizer in tokenizer_list:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenizer, ngram_range=(1, 2), max_df=0.85, min_df=2)\n",
    "    y_pred, y_score, df = evaluate_model(\n",
    "        vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "    tokenizer_y_score.append(y_score)    # Append y_score to the list\n",
    "\n",
    "# Plotting the ROC curve for each tokenizer\n",
    "plot_roc_curves([\"Simple Spacy Tokenizer\", \"Spacy Tokenizer\",\n",
    "                \"NLTK Tokenizer\"], tokenizer_y_score, 'Logistic Regression with different tokenizers')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model with a bit more advanced tokenizer has a slightly higher accuracy but the same ROC-AUC score. This means that the second tokenizer is slightly better at correctly classifying instances overall, but both models are almost equally good at distinguishing between the classes, as indicated by the ROC-AUC score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression with different vectorization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for Word2Vec to allow it to be used in a scikit-learn Pipeline\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [doc.split() for doc in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.size, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.model.wv[w] for w in doc.split() if w in self.model.wv]\n",
    "                                or [np.zeros(self.size)], axis=0) for doc in X])\n",
    "\n",
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer_ngrams)\n",
    "y_pred_c_vec, y_score_c_vec, c_vec_perf_df = evaluate_model(count_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer(tokenizer=spacy_tokenizer_ngrams)\n",
    "y_pred_h_vec, y_score_h_vec, h_vec_perf_df = evaluate_model(hashing_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Word2Vec\n",
    "w2v_vectorizer = Word2VecVectorizer()\n",
    "y_pred_w2v, y_score_w2v, w2v_perf_df = evaluate_model(w2v_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer_ngrams, ngram_range=(1,2), max_df=0.85, min_df=2)\n",
    "y_pred_tfidf, y_score_tfidf, tfidf_perf_df = evaluate_model(tfidf_vectorizer, clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve for each vectorizer\n",
    "plot_roc_curves([\"Count Vectorizer\", \"Hashing Vectorizer\", \"Word2Vec Vectorizer\", 'TF-IDF Vectorizer'],\n",
    "\t\t\t\t[y_score_c_vec, y_score_h_vec, y_score_w2v, y_score_tfidf], 'Logistic Regression with different Vectorizers')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the TF-IDF, CountVectorizer and HashingVectorizer perform equally well. We will fine-tune the hyperparameters of the TF-IDF vectorizer and use it in the next section. We see that the Word2Vec Vectorizer performs poorly which might be due to the small size of our dataset. We can see if it improves with the validation set, augemented data, and populated data from ChatGPT later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TF-IDF Vectorization and Logistic Regression Fine Tuning Hyperparameters\n",
    "\n",
    "In the initial model, we used a TF-IDF Vectorizer and a Logistic Regression classifier to predict whether a statement is an environmental claim or not. However, the model's performance can often be improved by tuning the hyperparameters of the vectorizer and the classifier.\n",
    "\n",
    "Hyperparameters are parameters that are not learned from the data. They are set prior to the commencement of the learning process. For instance, in the case of TF-IDF Vectorizer, `ngram_range`, `max_df`, and `min_df` are hyperparameters. For the Logistic Regression classifier, `C`, which is the inverse of regularization strength, is a hyperparameter. \n",
    "\n",
    "Hyperparameter tuning involves selecting the combination of hyperparameters for a machine learning model that performs the best on a validation set.\n",
    "\n",
    "#### Steps for Hyperparameter Tuning\n",
    "\n",
    "1. **Pipeline Creation**: We first created a pipeline that combines the vectorizer and the classifier. This allows us to jointly optimize the hyperparameters of both.\n",
    "\n",
    "2. **Define Hyperparameters**: We then defined a list of hyperparameters to tune for both the vectorizer and the classifier. For the vectorizer, we decided to tune `ngram_range`, `max_df`, and `min_df`. For the classifier, we decided to tune `C`.\n",
    "\n",
    "3. **Grid Search**: Next, we performed a grid search to find the combination of hyperparameters that results in the best cross-validated performance on the training data. Grid search works by training and evaluating a model for each combination of hyperparameters, and selecting the combination that performs best.\n",
    "\n",
    "4. **Best Parameters**: After the grid search, we printed the combination of hyperparameters that performed the best.\n",
    "\n",
    "5. **Evaluate the Model**: Finally, we used the best hyperparameters to create a new vectorizer and classifier, and evaluated the performance of the model on the test data.\n",
    "\n",
    "This process allowed us to optimize the model's performance by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new hyperparameter tuning function that doesn't include the tokenizer\n",
    "def hyperparameter_tuning(X_train, y_train, classifier):\n",
    "    ''' \n",
    "    Function to perform hyperparameter tuning for different classifiers\n",
    "    Input: X_train, y_train\n",
    "    Output: best_params_\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', TfidfVectorizer()), \n",
    "                     ('classifier', classifier)])\n",
    "    \n",
    "    # Define the hyperparameters to tune for LogisticRegression\n",
    "    if isinstance(classifier, LogisticRegression):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "        }\n",
    "         # Perform grid search\n",
    "        grid_search = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "        return grid_search.best_params_\n",
    "\n",
    "    # Define the hyperparameters to tune for RandomForest\n",
    "    elif isinstance(classifier, RandomForestClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters to tune for KNeighborsClassifier\n",
    "    elif isinstance(classifier, KNeighborsClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "            'classifier__weights': ['uniform', 'distance'],\n",
    "            'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "            'classifier__p': [1, 2]\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters for NN\n",
    "    elif isinstance(classifier, MLPClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['sgd', 'adam'],\n",
    "            'classifier__alpha': [0.0001, 0.05],\n",
    "            'classifier__learning_rate': ['constant','adaptive'],\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    " \n",
    "# Preprocess the data using the spaCy tokenizer\n",
    "X_train_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_train]   # Matching the GridSearchCV format\n",
    "X_test_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_test]     # Matching the GridSearchCV format\n",
    "\n",
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_tokenized, y_train, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "# Evaluate the model using previously made function\n",
    "y_pred_clf_tfidf_tuned, y_score_clf_tfidf_tuned, clf_tfidf_tuned_perf_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_tokenized, y_train, X_test_tokenized, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy improves with a little over 1% by fine-tuning the hyperparameters. But we still se that the simple CountVectorizer performed slightly better. We will see if including more data changes this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TF-IDF Vectorization and Logistic Regression with Balanced Data\n",
    "\n",
    "We saw previously that our data was heavily skewed, now let's try to run our logistic model with the balanced data created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_bal, y_train_bal, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_bal_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_bal_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "# Evaluate the model using previously made function\n",
    "y_pred_clf_tfidf_bal, y_score_clf_tfidf_bal, clf_tfidf_bal_perf = evaluate_model(vectorizer_bal_tuned, clf_bal_tuned, X_train_bal, y_train_bal, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the decrease in the training data evidently led to decrease in performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Text Classification using TF-IDF Vectorization and Logistic Regression with Generated Claims\n",
    "With populated data from ChatGPT3&4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chatgpt_train, y_chatgpt_train = claims_gpt_df['text'], claims_gpt_df['label']\n",
    "\n",
    "# Concatenate the train, validation and env_claims sets\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X_train_pop = pd.concat([X_train, X_chatgpt_train])\n",
    "y_train_pop = pd.concat([y_train, y_chatgpt_train])\n",
    "X_train_val_pop = pd.concat([X_train, X_val, X_chatgpt_train])\n",
    "y_train_val_pop = pd.concat([y_train, y_val, y_chatgpt_train])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set per class\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with populated claims\" + Colors.ENDC)\n",
    "display(y_train_pop.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation claims\" + Colors.ENDC)\n",
    "display(y_train_val.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation and populated claims\" + Colors.ENDC)\n",
    "display(y_train_val_pop.value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Trying our best LGR model with the additional validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_val, y_train_val, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_val_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_val_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "# Evaluating the performance of the validated data\n",
    "y_pred_clf_tfidf_val, y_score_clf_tfidf_val, clf_tfidf_val_perf_df = evaluate_model(vectorizer_val_tuned, clf_val_tuned, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't seem to improve our model at all. Still standing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Trying our best LGR model with the additional populated data (from ChatGPT3&4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating the performance of the populated data\n",
    "y_pred_clf_tfidf_pop, y_score_clf_tfidf_pop, clf_tfidf_pop_perf_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the CountVectorizer performs better with the populated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_clf_count_pop, y_score_clf_count_pop, clf_count_pop_perf_df = evaluate_model(count_vectorizer, clf_tuned, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the more sophisticated TF-IDF Vectorizer performs better than the simple CountVectorizer when we used the populated data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! it actually increased quite a lot! Cool, let's try to use them both and see what happens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Trying our best LGR model with the additional populated and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the populated and validated data\n",
    "y_pred_clf_tfidf_val_pop, y_score_clf_tfidf_val_pop, clf_tfidf_val_pop_per_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_val_pop, y_train_val_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the additional training-data from ChatGPT alone, gave the best performance. We used different prompts, asking for info from websites, annual reports and so on, in addition to differ in length (word count). This might have given the model more diversification and robustness, compared to the validation data which might be very similar to the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Trying our best LGR model with populated (ChatGPT3&4) and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(sentences, labels, augment_times=1):\n",
    "    '''\n",
    "    Function to perform data augmentation\n",
    "    Input: sentences - list of sentences\n",
    "           labels - list of labels corresponding to the sentences\n",
    "           augment_times - number of times to augment each sentence\n",
    "    Output: aug_sentences - list of augmented sentences\n",
    "            aug_labels - list of labels for the augmented sentences\n",
    "    '''\n",
    "    synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "    aug_sentences = []\n",
    "    aug_labels = []\n",
    "\n",
    "    for i in range(augment_times):\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            # Apply synonym augmentation\n",
    "            new_sentence = synonym_aug.augment(sentence)\n",
    "\n",
    "            # Only append the new sentence if it's not NaN\n",
    "            if pd.notnull(new_sentence):\n",
    "                aug_sentences.append(new_sentence)\n",
    "                aug_labels.append(label)\n",
    "\n",
    "    return aug_sentences, aug_labels\n",
    "\n",
    "# Only augmenting the label 1 sentences\n",
    "X_train_aug_1, y_train_aug_1 = data_augmentation(X_train[y_train == 1].values.tolist(), y_train[y_train == 1].values.tolist())\n",
    "\n",
    "# Convert the list of augmented sentences and labels to pandas Series\n",
    "X_train_aug_1 = pd.Series(X_train_aug_1, index = range(len(X_train), len(X_train) + len(X_train_aug_1)))\n",
    "y_train_aug_1 = pd.Series(y_train_aug_1, index = range(len(X_train), len(X_train) + len(y_train_aug_1)))\n",
    "\n",
    "# Feeding the data_augmentation function with the training data\n",
    "augmented_sentences, augmented_labels = data_augmentation(X_train.values.tolist(), y_train.values.tolist())\n",
    "\n",
    "# Convert the list of augmented sentences and labels to pandas Series\n",
    "augmented_sentences = pd.Series(augmented_sentences, index = range(len(X_train), len(X_train) + len(augmented_sentences)))\n",
    "augmented_labels = pd.Series(augmented_labels, index = range(len(X_train), len(X_train) + len(augmented_labels)))\n",
    "\n",
    "# Concatenate the augmented sentences and labels with the ChatGPT Populated and original training data\n",
    "X_train_aug = pd.concat([X_train_pop, augmented_sentences])\n",
    "y_train_aug = pd.concat([y_train_pop, augmented_labels])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with augmented data\" + Colors.ENDC)\n",
    "display(y_train_aug.value_counts())\n",
    "\n",
    "# Convert list of words in each document into a single string\n",
    "X_train_aug = [\" \".join(sublist) for sublist in X_train_aug]\n",
    "\n",
    "# Now we can evaluate the model\n",
    "y_pred_clf_tfidf_aug, y_score_clf_tfidf_aug, clf_tfidf_aug_per_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_aug, y_train_aug, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_data_input = [y_score_clf_tfidf_tuned, y_score_clf_tfidf_bal,\n",
    "                       y_score_clf_tfidf_val, y_score_clf_tfidf_pop, y_score_clf_tfidf_aug]\n",
    "data_input_names = ['BenchMark', 'Balanced', 'Validation',\n",
    "                    '+ ChatGPT', 'ChatGPT + Augmented']\n",
    "\n",
    "plot_roc_curves(data_input_names, y_scores_data_input,\n",
    "                'TF-IDF Logistic Regression for different Data Input')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. By comparing different ROC curves, we can make informed decisions about which vectorizer is best suited for our specific task based on its performance in terms of trade-off between sensitivity (TPR) and specificity (1 - FPR).\n",
    "\n",
    "Each line in the plot corresponds to a different data input. The closer a curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. This means the top left corner of the plot is the 'ideal' point - a false positive rate of zero, and a true positive rate of one. Therefore, a model whose ROC curve is closer to the top left corner performs better than a model whose curve is closer to the diagonal line.\n",
    "\n",
    "The diagonal line in the middle of the plot represents a random classifier (e.g., a coin flip), which has an equal chance of giving a correct or incorrect classification. Any good classifier should have its ROC curve above this line. If a curve is below this line, it means the classifier is worse than random chance. \n",
    "\n",
    "The dotted line represents a perfect classifier.\n",
    "\n",
    "Interestingly for the ChatGPT+Augmented data curve goes to 1 on the True Positive Rate scale before reaching 0.7 on the False Positive Rate, which means that our model is able to achieve a high rate of true positives (correctly identified positive instances) with a relatively low rate of false positives (negative instances incorrectly identified as positive). \n",
    "\n",
    "But when looking at other performance metrics such as accuracy, precision and recall, and area under curve in total, the ChatGPT data performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot the ROC as a bar chart\n",
    "def plot_roc_bars(names, y_scores, title):\n",
    "    '''\n",
    "    Function to plot the ROC curves as a bar chart\n",
    "    Input: names - list of names for the ROC curves\n",
    "           y_scores - list of y_scores for the ROC curves\n",
    "           title - title of the plot\n",
    "    Output: Plot of ROC curves\n",
    "    '''\n",
    "    # Create a list of vectorizer AUC scores\n",
    "    auc_scores = [roc_auc_score(y_test, y_score)\n",
    "                  for y_score in y_scores]\n",
    "\n",
    "    # Create a DataFrame of vectorizer performance\n",
    "    performance = pd.DataFrame(\n",
    "        {'Data': names, 'AUC Score': auc_scores})\n",
    "\n",
    "    # Sort the DataFrame by AUC score\n",
    "    performance.sort_values(\n",
    "        by='AUC Score', ascending=True, inplace=True, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='Data', y='AUC Score', data=performance,\n",
    "                palette='Blues')  # AUC = Area Under the Curve (ROC)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plotting the AUC scores on each bar\n",
    "    for index, row in performance.iterrows():\n",
    "        plt.text(index, row['AUC Score'] + 0.005,\n",
    "                 round(row['AUC Score'], 3), ha='center', color='black')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.xticks(fontsize=9)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plotting the ROC curves for the different data inputs\n",
    "plot_roc_bars(data_input_names, y_scores_data_input,\n",
    "                \"TF-IDF Logistic Regression Model Performance with various Data Input\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart provided offers a visual comparison of the performance of different vectorizers used in our text classification model, as measured by the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. AUC-ROC is a valuable metric for this purpose, as it provides a comprehensive view of model performance across all possible classification thresholds, unlike accuracy, precision, or recall which depend on a specific threshold. An AUC-ROC score close to 1.0 indicates that the model has a high ability to distinguish between the classes correctly, regardless of the threshold chosen. Therefore, in this graph, the TF-IDF vectorizer is the one that, on average, best discriminates between the classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import precison_recall_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Creating a functin that plots the relationship between the precision and the recall of several models\n",
    "def plot_precision_recall_curve(model_names, y_scores, title):\n",
    "\t'''\n",
    "\tFunction to plot the precision recall curve\n",
    "\tInput: model_names - list of model names\n",
    "\t\t   y_scores - list of y_scores for each model\n",
    "\t\t   title - title of the plot\n",
    "\tOutput: None\n",
    "\t'''\n",
    "\tplt.figure(figsize=(8, 6))\n",
    "\tfor model_name, y_score in zip(model_names, y_scores):\n",
    "\t\tprecision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "\t\tplt.plot(recall, precision, label=model_name)\n",
    "\n",
    "\t# Plotting the baseline\n",
    "\tplt.plot([0, 1], [0.5, 0.5], linestyle='--', label='Baseline')\n",
    "\n",
    "\tplt.xlabel('Recall')\n",
    "\tplt.ylabel('Precision')\n",
    "\tplt.title(title)\n",
    "\tplt.legend()\n",
    "\tplt.xlim(0, 1)\n",
    "\tplt.ylim(0, 1)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting the precision recall curve for the different data inputs\n",
    "plot_precision_recall_curve(data_input_names, y_scores_data_input, 'TF-IDF Logistic Regression Precision Recall Curve for different Data Input')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classifiers<a id=\"classifiers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying different classifiers and comparing their performance\n",
    "# Create a list of classifiers including KNN, Neural Net, LR, NB, RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), MLPClassifier(),\n",
    "               clf_tuned, MultinomialNB(), RandomForestClassifier()]\n",
    "\n",
    "vectorizer = [tfidf_vectorizer, w2v_vectorizer,\n",
    "              tfidf_vectorizer, count_vectorizer, tfidf_vectorizer]\n",
    "\n",
    "# Create a list of classifier names containing the names of the classifiers with the most suitable vectorizer\n",
    "classifier_names = ['KNN', 'Neural Net',\n",
    "                    'Logistic Regression', 'Naive Bayes', 'Random Forest']\n",
    "\n",
    "# Create a json to store the classifiers with the most suitable vectorizer\n",
    "classifiers_dict = dict(zip(classifier_names, classifiers))\n",
    "\n",
    "# Create a list of classifier predictions\n",
    "y_preds = []\n",
    "y_scores = []\n",
    "\n",
    "# Feeding all the prediction into the list of classifiers\n",
    "for classifier in classifiers:\n",
    "    y_pred, y_score, df = evaluate_model(\n",
    "        classifiers_dict[classifier], classifier, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "    # Creating a list of predictions with the classifier name to distinguish them\n",
    "    y_preds.append(y_pred)\n",
    "    y_scores.append(y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the ROC curves for the different classifiers\n",
    "plot_roc_curves(classifier_names, y_scores, \"Different Classifiers ROC Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_roc_bars(classifier_names, y_scores, \"Different Classifiers ROC\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest seems promising to further develop and tune."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fine Tuning the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_params_rf = hyperparameter_tuning(X_train, y_train, RandomForestClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest classifier using the best parameters\n",
    "clf_tuned_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params_rf['classifier__n_estimators'],\n",
    "    max_depth=best_params_rf['classifier__max_depth'],\n",
    "    min_samples_split=best_params_rf['classifier__min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['classifier__min_samples_leaf'],\n",
    "    bootstrap=best_params_rf['classifier__bootstrap'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_rf_tuned = TfidfVectorizer(ngram_range=best_params_rf['vectorizer__ngram_range'], \n",
    "                             max_df=best_params_rf['vectorizer__max_df'], min_df=best_params_rf['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned random forest classifier\n",
    "y_pred_clf_tuned_rf, y_score_clf_tuned_rf, rf_tuned_df = evaluate_model(\n",
    "    vectorizer_rf_tuned, clf_tuned_rf, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the RandomSearchGrid is too vague and therefor does not improve our model at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the optimal KNN classifier\n",
    "best_params_knn = hyperparameter_tuning(X_train_pop, y_train_pop, KNeighborsClassifier())\n",
    "\n",
    "# Create a new KNN classifier using the best parameters\n",
    "clf_tuned_knn = KNeighborsClassifier(\n",
    "    n_neighbors=best_params_knn['classifier__n_neighbors'],\n",
    "    weights=best_params_knn['classifier__weights'],\n",
    "    algorithm=best_params_knn['classifier__algorithm'],\n",
    "    leaf_size=best_params_knn['classifier__leaf_size'],\n",
    "    p=best_params_knn['classifier__p'])\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_knn_tuned = TfidfVectorizer(ngram_range=best_params_knn['vectorizer__ngram_range'],\n",
    "\t\t\t\t\t\t\t\t\t   max_df=best_params_knn['vectorizer__max_df'], min_df=best_params_knn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned KNN classifier\n",
    "y_pred_clf_tuned_knn, y_score_clf_tuned_knn = evaluate_model(\n",
    "    vectorizer_knn_tuned, clf_tuned_knn, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned KNN classifier's AUC score\n",
    "print('Tuned KNN Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_knn)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 NN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the optimal Neural Net classifier\n",
    "best_params_nn = hyperparameter_tuning(X_train_pop, y_train_pop, MLPClassifier())\n",
    "\n",
    "# Create a new Neural Net classifier using the best parameters\n",
    "clf_tuned_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params_nn['classifier__hidden_layer_sizes'],\n",
    "    activation=best_params_nn['classifier__activation'],\n",
    "    solver=best_params_nn['classifier__solver'],\n",
    "    alpha=best_params_nn['classifier__alpha'],\n",
    "    learning_rate=best_params_nn['classifier__learning_rate'],\n",
    "    max_iter=best_params_nn['classifier__max_iter'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_nn_tuned = TfidfVectorizer(ngram_range=best_params_nn['vectorizer__ngram_range'],\n",
    "                                        max_df=best_params_nn['vectorizer__max_df'], min_df=best_params_nn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned Neural Net classifier\n",
    "y_pred_clf_tuned_nn, y_score_clf_tuned_nn = evaluate_model(\n",
    "    vectorizer_nn_tuned, clf_tuned_nn, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned Neural Net classifier's AUC score\n",
    "print('Tuned Neural Net Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_nn)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Implementing voting classifier\n",
    "\n",
    "# Define a RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Define a MLPClassifier\n",
    "NN = MLPClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('RF', RF), ('NN', NN), ('LogReg', clf_tuned)], voting='hard')\n",
    "\n",
    "y_pred_vote, y_score_vote = evaluate_model(\n",
    "    vectorizer_tuned, voting_clf, X_train_pop, y_train_pop, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier()))\n",
    "level0.append(('NN', MLPClassifier()))\n",
    "level0.append(('LogReg', clf_tuned))\n",
    "\n",
    "# Define meta learner model\n",
    "level1 = clf_tuned\n",
    "\n",
    "# Define the stacking ensemble\n",
    "model_stacking = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "# Evaluate stacked model\n",
    "evaluate_model(vectorizer_tuned, model_stacking, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p pandas,numpy,sklearn,datasets,spacy,wordcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
