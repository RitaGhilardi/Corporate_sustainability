{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate Sustainability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Imports and dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, please make sure you have the following packages installed:\n",
    "\n",
    "- `pandas`: For handling data in dataframes.\n",
    "- `datasets`: For loading datasets.\n",
    "- `spacy`: For natural language processing.\n",
    "- `scikit-learn`: For machine learning algorithms, vectorization, model evaluation, and pipelines.\n",
    "- `wordcloud`: For creating a visual interpretation of text analytics\n",
    "\n",
    "You can install these packages using the following command:\n",
    "\n",
    "```bash\n",
    "pip install pandas datasets spacy scikit-learn wordcloud\n",
    "\n",
    "\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset('climatebert/environmental_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display to show how the format of the dataset looks like\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each split\n",
    "\n",
    "env_claim_train = pd.DataFrame(dataset['train'])\n",
    "env_claim_test = pd.DataFrame(dataset['test'])\n",
    "env_claim_val = pd.DataFrame(dataset['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color codes\n",
    "class Colors:\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "# Displaying the characteristics of the data\n",
    "print(Colors.OKGREEN + \"Training data\" + Colors.ENDC)\n",
    "print(f\"The shape of the training data is: {env_claim_train.shape}\")\n",
    "display(env_claim_train.head())\n",
    "display(env_claim_train.describe())\n",
    "\n",
    "print(Colors.OKBLUE + \"\\nTest data\" + Colors.ENDC)\n",
    "print(f\"The shape of the test data is: {env_claim_test.shape}\")\n",
    "display(env_claim_test.head())\n",
    "display(env_claim_test.describe())\n",
    "\n",
    "print(Colors.OKCYAN + \"\\nValidation data\" + Colors.ENDC)\n",
    "print(f\"The shape of the validation data is: {env_claim_val.shape}\")\n",
    "display(env_claim_val.head())\n",
    "display(env_claim_val.describe())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = env_claim_train['text'], env_claim_train['label']\n",
    "X_test, y_test = env_claim_test['text'], env_claim_test['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using TF-IDF Vectorization and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a spaCy tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.lemma_.lower().strip() for tok in nlp(text) if tok.is_alpha and not tok.is_stop]\n",
    "\n",
    "# Create a TfidfVectorizer with the spaCy tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 2), max_df=0.85, min_df=2)\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create a pipeline with the vectorizer and classifier\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('classifier', clf)])\n",
    "\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print the classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Importance with Word Clouds for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(classifier, vectorizer, class_index, class_label):\n",
    "    # Get the feature names (words) from the vectorizer\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get the coefficients (importance) of the words for the given class\n",
    "    word_importance = classifier.coef_[0]\n",
    "\n",
    "    # Create a dictionary with the words and their importance\n",
    "    if class_index == 0:\n",
    "        colormap = \"Reds\"\n",
    "        word_importance_dict = {words[i]: -word_importance[i] for i in range(len(words)) if word_importance[i] < 0}\n",
    "    else:\n",
    "        word_importance_dict = {words[i]: word_importance[i] for i in range(len(words)) if word_importance[i] > 0}\n",
    "        colormap = \"Greens\"\n",
    "\n",
    "    # Create a WordCloud object\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=colormap)\n",
    "\n",
    "    # Generate the word cloud using the word importance dictionary\n",
    "    wc.generate_from_frequencies(word_importance_dict)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for {class_label}\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# Create the word clouds for each class\n",
    "plot_word_cloud(clf, vectorizer, 0, \"Greenwashing Companies\")\n",
    "plot_word_cloud(clf, vectorizer, 1, \"Genuine Green impact Companies\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p pandas,numpy,sklearn,datasets,spacy,wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
