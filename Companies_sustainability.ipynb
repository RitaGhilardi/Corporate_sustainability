{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate Sustainability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Imports and Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, please make sure you have the following packages installed:\n",
    "\n",
    "- `pandas`: For handling data in dataframes.\n",
    "- `datasets`: For loading datasets.\n",
    "- `spacy`: For natural language processing.\n",
    "- `scikit-learn`: For machine learning algorithms, vectorization, model evaluation, and pipelines.\n",
    "- `wordcloud`: For creating a visual interpretation of text analytics.\n",
    "- `nlpaug`: For data augmenting\n",
    "\n",
    "You can install these packages using the following command:\n",
    "\n",
    "```bash\n",
    "pip install pandas datasets spacy scikit-learn wordcloud nlpaug\n",
    "\n",
    "```\n",
    "or using conda:\n",
    "```bash\n",
    "conda install pandas datasets spacy scikit-learn wordcloud nlpaug\n",
    "\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             HashingVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             confusion_matrix, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset('climatebert/environmental_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display to show how the format of the dataset looks like\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each split\n",
    "\n",
    "env_claim_train = pd.DataFrame(dataset['train'])\n",
    "env_claim_test = pd.DataFrame(dataset['test'])\n",
    "env_claim_val = pd.DataFrame(dataset['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color codes\n",
    "class Colors:\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "# Displaying the characteristics of the data\n",
    "print(Colors.OKGREEN + \"Training data\" + Colors.ENDC)\n",
    "print(f\"The shape of the training data is: {env_claim_train.shape}\")\n",
    "display(env_claim_train.head())\n",
    "display(env_claim_train.label.value_counts())\n",
    "\n",
    "print(Colors.OKBLUE + \"\\nTest data\" + Colors.ENDC)\n",
    "print(f\"The shape of the test data is: {env_claim_test.shape}\")\n",
    "display(env_claim_test.head())\n",
    "display(env_claim_test.label.value_counts())\n",
    "\n",
    "print(Colors.OKCYAN + \"\\nValidation data\" + Colors.ENDC)\n",
    "print(f\"The shape of the validation data is: {env_claim_val.shape}\")\n",
    "display(env_claim_val.head())\n",
    "display(env_claim_val.label.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets \n",
    "claim_dataset = pd.concat([env_claim_train, env_claim_test, env_claim_val], ignore_index = True)\n",
    "print(\"Number of claims in the dataset:\", claim_dataset.shape[0])    # observations\n",
    "print(\"Number of variables in the dataset:\", claim_dataset.shape[1]) # variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs \n",
    "print(\"Number of NaNs:\")\n",
    "display(claim_dataset.isna().sum())\n",
    "\n",
    "# Duplicates\n",
    "print(\"Number of duplicates:\")\n",
    "display(claim_dataset.duplicated().sum())\n",
    "\n",
    "# Variable types\n",
    "print(\"Variable types:\")\n",
    "claim_dataset.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count by Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "claim_dataset[\"word count\"] = claim_dataset[\"text\"].apply(lambda x: len(x.split()))\n",
    "print(\"The average number of words in each claim is equal to:\", round(claim_dataset[\"word count\"].mean(),0), \"words.\")\n",
    "\n",
    "# Graphical representation by class\n",
    "class_1_counts = claim_dataset[claim_dataset[\"label\"] == 1][\"word count\"]\n",
    "class_2_counts = claim_dataset[claim_dataset[\"label\"] == 0][\"word count\"]\n",
    "\n",
    "plt.hist(class_1_counts, bins = range(11, 39), alpha = 0.5, label = \"Environmental Claim\", color = \"#4958B5\")\n",
    "plt.hist(class_2_counts, bins = range(11, 39), alpha = 0.5, label = \"Non-environmental Claim\", color = \"#8DB8B7\")\n",
    "plt.xlabel(\"Word count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Number of Words in Each Claim by Class\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the Spacy sp function to each row of the 'text' column\n",
    "claim_dataset[\"spacy object\"] = claim_dataset[\"text\"].apply(sp)\n",
    "\n",
    "# Filter stopwords, punctuation and spaces\n",
    "def filter_tokens(token):\n",
    "    return not token.is_stop and not token.is_punct and not token.is_space\n",
    "\n",
    "# Remove stopwords, punctuation, and whitespace from each Spacy object\n",
    "claim_dataset[\"filtered tokens\"] = claim_dataset[\"spacy object\"].apply(lambda doc: [token.text for token in doc if filter_tokens(token)])\n",
    "\n",
    "print(\"This is the first sentence before filtering:\", claim_dataset.iloc[0,0])\n",
    "print(\"\\nThis is the first sentence after filtering:\", claim_dataset.iloc[0,4])\n",
    "\n",
    "# Calculating new average value of words per claim\n",
    "number_words = [len(x) for x in claim_dataset[\"filtered tokens\"]]\n",
    "print(\"\\nThe average number of words per claim is now:\", round(np.mean(number_words),0))\n",
    "display(claim_dataset.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Claims versus Non-Environmental Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean \n",
    "print(\"Average number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").mean().round())\n",
    "\n",
    "# Median\n",
    "print(\"\\nMedian number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud hue by class label\n",
    "\n",
    "# Join the strings in each list into a single string\n",
    "claim_dataset[\"joined tokens\"] = claim_dataset[\"filtered tokens\"].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# For Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==1][\"joined tokens\"])\n",
    "wordcloud = WordCloud( background_color = \"white\", colormap = \"Greens\").generate(text)\n",
    "\n",
    "ax1.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "ax1.set(title = \"WordCloud of Environmental Claims\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# For Non-Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==0][\"joined tokens\"])\n",
    "wordcloud = WordCloud(background_color = \"white\", colormap = \"Reds\").generate(text)\n",
    "\n",
    "ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "ax2.set(title = \"WordCloud of Non-Environmental Claims\")\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words for environmental claims\n",
    "top = Counter([item for sublist in claim_dataset[\"joined tokens\"][claim_dataset[\"label\"]==1] for item in str(sublist).split()])\n",
    "temp = pd.DataFrame(top.most_common(10))\n",
    "temp.columns = [\"Common Words\", \"Count\"]\n",
    "print(\"Most frequent words for environmental claims:\")\n",
    "display(temp.style.background_gradient(cmap = \"Greens\"))\n",
    "\n",
    "# Most frequent words for non-environmental claims\n",
    "top = Counter([item for sublist in claim_dataset[\"joined tokens\"][claim_dataset[\"label\"]==0] for item in str(sublist).split()])\n",
    "temp = pd.DataFrame(top.most_common(10))\n",
    "temp.columns = [\"Common Words\", \"Count\"]\n",
    "print(\"\\nMost frequent words for non-environmental claims:\")\n",
    "display(temp.style.background_gradient(cmap = \"Reds\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Populating with more environmental claims\n",
    "\n",
    "We have used ChatGPT 3 and 4 to generate different environmental claims. These can be found in the `env_claims_gpt3.json` and `env_claims_gpt4.json` in the `./data` folder.\n",
    "\n",
    "We will append these statements to the ones we already have, and later investigate whether these can improve our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the env claims from env_claims_gpt4.json\n",
    "\n",
    "claims_gpt4 = []\n",
    "# Load the env_claims_gpt3.json file\n",
    "with open('./data/env_claims_gpt4.json') as json_file:\n",
    "\tdata = json.load(json_file)\n",
    "\n",
    "\tfor i in range(len(data['claims'])):\n",
    "\t\tclaims_gpt4.append(data['claims'][i]['claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_gpt3 = []\n",
    "# Load the env_claims_gpt3.json file\n",
    "with open('./data/env_claims_gpt3.json') as json_file:\n",
    "\tdata = json.load(json_file)\n",
    "\n",
    "\tfor i in range(len(data['claims'])):\n",
    "\t\tclaims_gpt3.append(data['claims'][i]['claim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of claims\n",
    "print(\"Number of claims in the dataset (GPT4):\", len(claims_gpt4))    # observations\n",
    "print(\"Number of claims in the dataset (GPT3):\", len(claims_gpt3))    # observations\n",
    "print(\"Total number of claims in the dataset:\", len(claims_gpt4)+len(claims_gpt3))    # observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list to a dataframe\n",
    "\n",
    "claims_gpt4_df = pd.DataFrame(claims_gpt4, columns = ['text'])\n",
    "claims_gpt3_df = pd.DataFrame(claims_gpt3, columns = ['text'])\n",
    "\n",
    "# Adding the label column\n",
    "claims_gpt4_df['label'] = 1\n",
    "claims_gpt3_df['label'] = 1\n",
    "\n",
    "# Concatenating the two dataframes\n",
    "claims_gpt_df = pd.concat([claims_gpt4_df, claims_gpt3_df], ignore_index=True)\n",
    "display(claims_gpt_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Energy Claims using N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to finish this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample of claims with word \"energy\" inside\n",
    "energy_df = claim_dataset[claim_dataset[\"joined tokens\"].str.contains(\"energy\")]\n",
    "print(\"In the original dataset, there are\", len(energy_df), \"claims containing the word 'energy'.\")\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 1\n",
    "energy_df_1 = energy_df[energy_df[\"label\"] == 1]\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 0\n",
    "energy_df_0 = energy_df[energy_df[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculating most frequent N-Grams given corpus, n-grams\n",
    "\n",
    "def top_n_ngram(energy_corpus, ngram = 3):\n",
    "    vec = CountVectorizer(ngram_range = (ngram,ngram)).fit(energy_corpus)\n",
    "    words_bag = vec.transform(energy_corpus)  # Have the count of  all the words for each claim\n",
    "    sum_words = words_bag.sum(axis = 0)       # Calculates the count of all the word in the whole claim\n",
    "    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n",
    "    return words_freq\n",
    "\n",
    "# Call function on both datasets \n",
    "pop_words_1 = top_n_ngram(energy_df_1[\"joined tokens\"], 3)  \n",
    "pop_words_0 = top_n_ngram(energy_df_0[\"joined tokens\"], 3)  \n",
    "\n",
    "# Select top 20 N-Grams having 'energy' in text \n",
    "pop_energy_1 = [t for t in pop_words_1 if \"energy\" in t[0]]\n",
    "pop_energy_1 = pop_energy_1[:20]\n",
    "pop_energy_0 = [t for t in pop_words_0 if \"energy\" in t[0]]\n",
    "pop_energy_0 = pop_energy_0[:20]\n",
    "\n",
    "# Graphical representation\n",
    "\n",
    "# Extract x and y values from each list\n",
    "x1, y1 = zip(*pop_energy_1)\n",
    "x2, y2 = zip(*pop_energy_0)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 2, figsize = (20,9))\n",
    "fig.subplots_adjust(hspace = 1.2)\n",
    "\n",
    "# Create the first bar plot on ax1\n",
    "ax1.bar(x1, y1, color = \"#4958B5\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xticks(range(len(x1)))\n",
    "ax1.set_xticklabels(x1, rotation = 90)\n",
    "ax1.set_title(\"Top 20 Energy 3-Grams in Environmental Claims\")\n",
    "\n",
    "# Create the second bar plot on ax2\n",
    "ax2.bar(x2, y2, color = \"#8DB8B7\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xticks(range(len(x2)))\n",
    "ax2.set_xticklabels(x2, rotation = 90)\n",
    "ax2.set_title(\"Top 20 Energy 3-Grams in Non-Environmental Claims\")\n",
    "\n",
    "# Add x-axis label\n",
    "fig.add_subplot(111, frameon = False)\n",
    "plt.tick_params(labelcolor = \"none\", top = False, bottom = False, left = False, right = False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes for the team: I did the study of the previous section to understand why energy is the most frequent word\n",
    "# in both claims (environmental vs. non-environmental). With the 3 n-grams I wanted to see whether I could\n",
    "# find a difference in how they use the word \"energy\" but nothing strikes me actually. If you have any idea on \n",
    "# how I could further explore it or explain the difference I'd be happy to listed at your suggestions \n",
    "\n",
    "# Please let me know if you would add anything else to the EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the train and test datasets for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = env_claim_train['text'], env_claim_train['label']\n",
    "X_test, y_test = env_claim_test['text'], env_claim_test['label']\n",
    "X_val, y_val = env_claim_val['text'], env_claim_test['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Labelled Data and Defining Base Rate\n",
    "\n",
    "TODO: Include gpt generated claims and show graph for both dataframes and label categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class:\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "      \n",
    "\n",
    "# Test Set \n",
    "print(Colors.OKBLUE + \"\\nTest set per class:\" + Colors.ENDC)\n",
    "display(y_test.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class:\" + Colors.ENDC)\n",
    "display(y_val.value_counts())\n",
    "\n",
    "\n",
    "# Graphical representation\n",
    "outcome_variable = pd.concat([y_train, y_test, y_val])\n",
    "outcome_variable.value_counts().plot.bar(color = [\"#4958B5\", \"#8DB8B7\"], grid = False)\n",
    "plt.ylabel(\"Number of observations\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.title(\"Number of Observations per Class\")\n",
    "plt.xticks(rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base rate\n",
    "base_rate = round(len(outcome_variable[outcome_variable == 0]) / len (outcome_variable), 4)\n",
    "print(f'The base rate is: {base_rate*100:0.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of \"0\" outcomes in the training set\n",
    "train_zeros_idx = pd.Series(y_train[y_train == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "train_zeros_sample_idx = train_zeros_idx.sample(n = sum(y_train == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced training set\n",
    "X_train_bal = pd.concat([X_train[y_train == 1], X_train[train_zeros_sample_idx]])\n",
    "y_train_bal = pd.concat([y_train[y_train == 1], y_train[train_zeros_sample_idx]])\n",
    "\n",
    "\n",
    "# Get indices of \"0\" outcomes in the test set\n",
    "test_zeros_idx = pd.Series(y_test[y_test == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "test_zeros_sample_idx = test_zeros_idx.sample(n = sum(y_test == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced test set\n",
    "X_test_bal = pd.concat([X_test[y_test == 1], X_test[test_zeros_sample_idx]])\n",
    "y_test_bal = pd.concat([y_test[y_test == 1], y_test[test_zeros_sample_idx]])\n",
    "\n",
    "# Get indices of \"0\" outcomes in the validation set\n",
    "val_zeros_idx = pd.Series(y_val[y_val == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "val_zeros_sample_idx = val_zeros_idx.sample(n = sum(y_val == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced validation set\n",
    "X_val_bal = pd.concat([X_val[y_val == 1], X_val[val_zeros_sample_idx]])\n",
    "y_val_bal = pd.concat([y_val[y_val == 1], y_val[val_zeros_sample_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations per class after balancing the classes:\\n\")\n",
    "\n",
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class\" + Colors.ENDC)\n",
    "display(y_train_bal.value_counts())\n",
    "      \n",
    "\n",
    "# Test Set \n",
    "print(Colors.OKBLUE + \"\\nTest set per class\" + Colors.ENDC)\n",
    "display(y_test_bal.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class\" + Colors.ENDC)\n",
    "display(y_val_bal.value_counts())\n",
    "\n",
    "print(\"\\nThe new balanced dataset contains\", len(y_train_bal + y_test_bal + y_val_bal) , \"observations.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training and running ML model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text Classification using TF-IDF Vectorization and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.lemma_.lower().strip() for tok in nlp(text) if tok.is_alpha and not tok.is_stop]\n",
    "\n",
    "def classification_report_prettify(report):\n",
    "    ''' \n",
    "    Creates a df from the precision_recall_fscore_support function\n",
    "    Input: model performance metrics\n",
    "    '''\n",
    "    out_df = pd.DataFrame(report).transpose()\n",
    "    out_df.columns = ['precision', 'recall', 'f1-score', 'support']\n",
    "    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n",
    "    avg_tot.index = [\"avg/total\"]\n",
    "    out_df = pd.concat([out_df, avg_tot])\n",
    "    out_df['support'] = out_df['support'].apply(lambda x: int(x))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_score):\n",
    "    '''\n",
    "    This function plots the ROC curve.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    '''\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=\"Blues\", cbar=False)\n",
    "    \n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(vectorizer, classifier, X_train, y_train, X_test, y_test, with_confusion_matrix=False):\n",
    "    ''' \n",
    "    Function to evaluate the model performance\n",
    "    Input: vectorizer, classifier, X_train, y_train, X_test, y_test\n",
    "    Output: accuracy score and classification report\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "\n",
    "    # Train the model\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    y_score = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Adding a title to the dataframe based on the model and vectorizer used\n",
    "    title = str(classifier).split('(')[0] + ' with ' + str(vectorizer).split('(')[0]\n",
    "    print(\"-\" * len(title))\n",
    "    print(Colors.OKGREEN + title + Colors.ENDC)\n",
    "    print(\"-\" * len(title))\n",
    "\n",
    "    # Calculate accuracy and print the performance metrics\n",
    "    display(classification_report_prettify(precision_recall_fscore_support(y_test, y_pred)))\n",
    "    accuracy = accuracy_score(y_test, y_pred)   # Accuracy is the number of correct predictions divided by the total number of predictions\n",
    "    print(Colors.OKBLUE + f'Accuracy: {accuracy*100:0.2f}%' + Colors.ENDC)    # Print the accuracy with 3 decimal points\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if with_confusion_matrix:\n",
    "        plot_confusion_matrix(y_test, y_pred, ['0', '1'])\n",
    "        \n",
    "    return y_pred, y_score\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 2), max_df=0.85, min_df=2)  # Define the vectorizer\n",
    "clf = LogisticRegression(solver='liblinear')                                                        # Define the classifier\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_clf_tfidf, y_score_clf_tfidf = evaluate_model(vectorizer, clf, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_pred_clf_tfidf, y_score_clf_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF Vectorization and Logistic Regression Fine Tuning Hyperparameters\n",
    "\n",
    "In the initial model, we used a TF-IDF Vectorizer and a Logistic Regression classifier to predict whether a statement is an environmental claim or not. However, the model's performance can often be improved by tuning the hyperparameters of the vectorizer and the classifier.\n",
    "\n",
    "Hyperparameters are parameters that are not learned from the data. They are set prior to the commencement of the learning process. For instance, in the case of TF-IDF Vectorizer, `ngram_range`, `max_df`, and `min_df` are hyperparameters. For the Logistic Regression classifier, `C`, which is the inverse of regularization strength, is a hyperparameter. \n",
    "\n",
    "Hyperparameter tuning involves selecting the combination of hyperparameters for a machine learning model that performs the best on a validation set.\n",
    "\n",
    "#### Steps Taken for Hyperparameter Tuning\n",
    "\n",
    "1. **Pipeline Creation**: We first created a pipeline that combines the vectorizer and the classifier. This allows us to jointly optimize the hyperparameters of both.\n",
    "\n",
    "2. **Define Hyperparameters**: We then defined a list of hyperparameters to tune for both the vectorizer and the classifier. For the vectorizer, we decided to tune `ngram_range`, `max_df`, and `min_df`. For the classifier, we decided to tune `C`.\n",
    "\n",
    "3. **Grid Search**: Next, we performed a grid search to find the combination of hyperparameters that results in the best cross-validated performance on the training data. Grid search works by training and evaluating a model for each combination of hyperparameters, and selecting the combination that performs best.\n",
    "\n",
    "4. **Best Parameters**: After the grid search, we printed the combination of hyperparameters that performed the best.\n",
    "\n",
    "5. **Evaluate the Model**: Finally, we used the best hyperparameters to create a new vectorizer and classifier, and evaluated the performance of the model on the test data.\n",
    "\n",
    "This process allowed us to optimize the model's performance by finding the best hyperparameters. Note that the best hyperparameters are specific to the training data, and might not be the best for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using the spaCy tokenizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "X_train_tokenized = [' '.join(spacy_tokenizer(text)) for text in X_train]   # Matching the GridSearchCV format\n",
    "X_test_tokenized = [' '.join(spacy_tokenizer(text)) for text in X_test]     # Matching the GridSearchCV format\n",
    "\n",
    "# Define a new hyperparameter tuning function that doesn't include the tokenizer\n",
    "def hyperparameter_tuning(X_train, y_train, classifier):\n",
    "    ''' \n",
    "    Function to perform hyperparameter tuning\n",
    "    Input: X_train, y_train\n",
    "    Output: best_params_\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', TfidfVectorizer()), \n",
    "                     ('classifier', classifier)])\n",
    "    \n",
    "    # Define the hyperparameters to tune for LogisticRegression\n",
    "    if isinstance(classifier, LogisticRegression):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "        }\n",
    "         # Perform grid search\n",
    "        grid_search = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "        return grid_search.best_params_\n",
    "\n",
    "    # Define the hyperparameters to tune for RandomForest\n",
    "    elif isinstance(classifier, RandomForestClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters to tune for KNeighborsClassifier\n",
    "    elif isinstance(classifier, KNeighborsClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "            'classifier__weights': ['uniform', 'distance'],\n",
    "            'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "            'classifier__p': [1, 2]\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters for NN\n",
    "    elif isinstance(classifier, MLPClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['sgd', 'adam'],\n",
    "            'classifier__alpha': [0.0001, 0.05],\n",
    "            'classifier__learning_rate': ['constant','adaptive'],\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    " \n",
    "\n",
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_tokenized, y_train, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "clf_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "# Evaluate the model using previously made function\n",
    "y_pred_clf_tfidf_tuned, y_score_clf_tfidf_tuned = evaluate_model(vectorizer_tuned, clf_tuned, X_train_tokenized, y_train, X_test_tokenized, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy improves with a little over 1% by fine-tuning the hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Text Classification using TF-IDF Vectorization and Logistic Regression With Balanced Data\n",
    "\n",
    "We saw previously that our data was heavily skewed, now let's try to run our logistic model with the balanced data created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the balanced data\n",
    "y_pred_clf_tfidf_bal, y_score_clf_tfidf_bal = evaluate_model(vectorizer_tuned, clf_tuned, X_train_bal, y_train_bal, X_test_bal, y_test_bal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the decrease in the training data evidently led to decrease in performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Text Classification using TF-IDF Vectorization and Logistic Regression with Generated Claims\n",
    "With populated data from ChatGPT3&4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chatgpt_train, y_chatgpt_train = claims_gpt_df['text'], claims_gpt_df['label']\n",
    "\n",
    "# Concatenate the train, validation and env_claims sets\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X_train_pop = pd.concat([X_train, X_chatgpt_train])\n",
    "y_train_pop = pd.concat([y_train, y_chatgpt_train])\n",
    "X_train_val_pop = pd.concat([X_train, X_val, X_chatgpt_train])\n",
    "y_train_val_pop = pd.concat([y_train, y_val, y_chatgpt_train])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set per class\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with populated claims\" + Colors.ENDC)\n",
    "display(y_train_pop.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation claims\" + Colors.ENDC)\n",
    "display(y_train_val.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation and populated claims\" + Colors.ENDC)\n",
    "display(y_train_val_pop.value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Trying our best LGR model with the additional validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the validated data\n",
    "y_pred_clf_tfidf_val, y_score_clf_tfidf_val = evaluate_model(vectorizer_tuned, clf_tuned, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Trying our best LGR model with the additional populated data (from ChatGPT3&4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the populated data\n",
    "y_pred_clf_tfidf_pop, y_score_clf_tfidf_pop = evaluate_model(vectorizer_tuned, clf_tuned, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! it actually increased quite a lot! Cool, let's try to use them both and see what happens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Trying our best LGR model with the additional populated and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the populated and validated data\n",
    "y_pred_clf_tfidf_val_pop, y_score_clf_tfidf_val_pop = evaluate_model(vectorizer_tuned, clf_tuned, X_train_val_pop, y_train_val_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the additional training-data from ChatGPT alone, gave the best performance. We used different prompts, asking for info from websites, annual reports and so on, in addition to differ in length. This might have given the model more diversification and robustness, compared to the validation data which might be very similar to the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4 Trying our best LGR model with populated (ChatGPT3&4) and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(sentences, labels, augment_times=1):\n",
    "    '''\n",
    "    Function to perform data augmentation\n",
    "    Input: sentences - list of sentences\n",
    "           labels - list of labels corresponding to the sentences\n",
    "           augment_times - number of times to augment each sentence\n",
    "    Output: aug_sentences - list of augmented sentences\n",
    "            aug_labels - list of labels for the augmented sentences\n",
    "    '''\n",
    "    synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "    aug_sentences = []\n",
    "    aug_labels = []\n",
    "\n",
    "    for i in range(augment_times):\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            # Apply synonym augmentation\n",
    "            new_sentence = synonym_aug.augment(sentence)\n",
    "\n",
    "            # Only append the new sentence if it's not NaN\n",
    "            if pd.notnull(new_sentence):\n",
    "                aug_sentences.append(new_sentence)\n",
    "                aug_labels.append(label)\n",
    "\n",
    "    return aug_sentences, aug_labels\n",
    "\n",
    "\n",
    "# Feeding the data_augmentation function with the training data\n",
    "augmented_sentences, augmented_labels = data_augmentation(X_train_pop.values.tolist(), y_train.values.tolist())\n",
    "\n",
    "# Convert the list of augmented sentences and labels to pandas Series\n",
    "augmented_sentences = pd.Series(augmented_sentences, index = range(len(X_train_pop), len(X_train_pop) + len(augmented_sentences)))\n",
    "augmented_labels = pd.Series(augmented_labels, index = range(len(X_train_pop), len(X_train_pop) + len(augmented_labels)))\n",
    "\n",
    "# Concatenate the augmented sentences and labels with the original training data\n",
    "X_train_aug = pd.concat([X_train_pop, augmented_sentences])\n",
    "y_train_aug = pd.concat([y_train_pop, augmented_labels])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with augmented data\" + Colors.ENDC)\n",
    "display(y_train_aug.value_counts())\n",
    "\n",
    "# Convert list of words in each document into a single string\n",
    "X_train_aug = [\" \".join(sublist) for sublist in X_train_aug]\n",
    "\n",
    "# Now we can evaluate the model\n",
    "y_pred_clf_tfidf_aug, y_score_clf_tfidf_aug = evaluate_model(vectorizer_tuned, clf_tuned, X_train_aug, y_train_aug, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Trying different text embeddings with the fine tuned logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for Word2Vec to allow it to be used in a scikit-learn Pipeline\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [doc.split() for doc in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.size, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.model.wv[w] for w in doc.split() if w in self.model.wv]\n",
    "                                or [np.zeros(self.size)], axis=0) for doc in X])\n",
    "\n",
    "# Now, you can use these vectorizers with the evaluate_model function:\n",
    "\n",
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "y_pred_c_vec, y_score_c_vec = evaluate_model(count_vectorizer, clf_tuned, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer(tokenizer=spacy_tokenizer)\n",
    "y_pred_h_vec, y_score_h_vec = evaluate_model(hashing_vectorizer, clf_tuned, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Word2Vec\n",
    "w2v_vectorizer = Word2VecVectorizer()\n",
    "y_pred_w2v, y_score_w2v = evaluate_model(w2v_vectorizer, clf_tuned, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Comparing Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of vectorizer names\n",
    "vectorizer_names = ['Count Vectorizer', 'Hashing Vectorizer', 'Word2Vec Vectorizer', 'TF-IDF Vectorizer']\n",
    "\n",
    "# Create a list of vectorizer scores\n",
    "y_scores = [y_score_c_vec, y_score_h_vec, y_score_w2v, y_score_clf_tfidf_pop]\n",
    "\n",
    "# Comparing the ROC curves of the vectorizers\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve for each vectorizer\n",
    "for name, y_score in zip(vectorizer_names, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "\n",
    "# Plot the ROC curve of a purely random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "# Plot the ROC curve of a perfect classifier\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'k:', label='Perfect Classifier')\n",
    "\n",
    "# Add labels and legend to the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves of Vectorizers')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot, known as the Receiver Operating Characteristic (ROC) curve, is an important tool for evaluating the performance of binary classifiers. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. By comparing different ROC curves, we can make informed decisions about which vectorizer is best suited for our specific task based on its performance in terms of trade-off between sensitivity (TPR) and specificity (1 - FPR).\n",
    "\n",
    "Each line in the plot corresponds to a different vectorizer. The closer a curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. This means the top left corner of the plot is the 'ideal' point - a false positive rate of zero, and a true positive rate of one. Therefore, a model whose ROC curve is closer to the top left corner performs better than a model whose curve is closer to the diagonal line.\n",
    "\n",
    "The diagonal line in the middle of the plot represents a random classifier (e.g., a coin flip), which has an equal chance of giving a correct or incorrect classification. Any good classifier should have its ROC curve above this line. If a curve is below this line, it means the classifier is worse than random chance. \n",
    "\n",
    "The dotted line represents a perfect classifier.\n",
    "\n",
    "In conclusion, this ROC curve comparison plot is useful because it provides a comprehensive visual representation of how well each vectorizer performs as a classifier, considering both types of errors (false positives and false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of vectorizer predictions\n",
    "y_preds = [y_pred_c_vec, y_pred_h_vec, y_pred_w2v, y_pred_clf_tfidf_pop]\n",
    "\n",
    "# Create a list of vectorizer AUC scores\n",
    "auc_scores = [roc_auc_score(y_test, y_score) for y_score in y_scores]\n",
    "\n",
    "# Create a DataFrame of vectorizer performance\n",
    "vectorizer_performance = pd.DataFrame(\n",
    "    {'Vectorizer': vectorizer_names, 'AUC Score': auc_scores})\n",
    "\n",
    "# Plot the performance of the vectorizers as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Vectorizer', y='AUC Score', data=vectorizer_performance,\n",
    "            palette='Blues')  # AUC = Area Under the Curve (ROC)\n",
    "plt.title('Vectorizer Performance')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart provided offers a visual comparison of the performance of different vectorizers used in our text classification model, as measured by the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. AUC-ROC is a valuable metric for this purpose, as it provides a comprehensive view of model performance across all possible classification thresholds, unlike accuracy, precision, or recall which depend on a specific threshold. An AUC-ROC score close to 1.0 indicates that the model has a high ability to distinguish between the classes correctly, regardless of the threshold chosen. Therefore, in this graph, the TF-IDF vectorizer is the one that, on average, best discriminates between the classes in our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying different classifiers and comparing their performance\n",
    "\n",
    "# Create a list of classifiers including KNN, Neural Net, LR, NB, RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), MLPClassifier(),\n",
    "               clf_tuned, MultinomialNB(), RandomForestClassifier()]\n",
    "\n",
    "# Create a list of classifier names\n",
    "classifier_names = ['KNN', 'Neural Net',\n",
    "                    'LogReg Tuned', 'Naive Bayes', 'Random Forest']\n",
    "\n",
    "# Create a list of classifier predictions\n",
    "y_preds = []\n",
    "y_scores = []\n",
    "\n",
    "# Feeding all the prediction into the list of classifiers\n",
    "for classifier in classifiers:\n",
    "    y_pred, y_score = evaluate_model(\n",
    "        vectorizer_tuned, classifier, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "    # Creating a list of predictions with the classifier name to distinguish them\n",
    "    y_preds.append(y_pred)\n",
    "    y_scores.append(y_score)\n",
    "\n",
    "\n",
    "# Create a list of classifier AUC scores\n",
    "auc_scores = [roc_auc_score(y_test, y_score) for y_score in y_preds]\n",
    "\n",
    "# Create a DataFrame of classifier performance\n",
    "classifier_performance = pd.DataFrame(\n",
    "    {'Classifier': classifier_names, 'AUC Score': auc_scores})\n",
    "\n",
    "# Plot the performance of the classifiers as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Classifier', y='AUC Score', data=classifier_performance,\n",
    "            palette='Blues')  # AUC = Area Under the Curve (ROC)\n",
    "plt.title('Classifier Performance')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest seems promising to further develop and tune."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fine Tuning the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf = hyperparameter_tuning(X_train_pop, y_train_pop, RandomForestClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest classifier using the best parameters\n",
    "clf_tuned_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params_rf['classifier__n_estimators'],\n",
    "    max_depth=best_params_rf['classifier__max_depth'],\n",
    "    min_samples_split=best_params_rf['classifier__min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['classifier__min_samples_leaf'],\n",
    "    bootstrap=best_params_rf['classifier__bootstrap'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_rf_tuned = TfidfVectorizer(ngram_range=best_params_rf['vectorizer__ngram_range'], \n",
    "                             max_df=best_params_rf['vectorizer__max_df'], min_df=best_params_rf['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned random forest classifier\n",
    "y_pred_clf_tuned_rf, y_score_clf_tuned_rf = evaluate_model(\n",
    "    vectorizer_rf_tuned, clf_tuned_rf, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned random forest classifier's AUC score\n",
    "print('Tuned Random Forest Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_rf)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the RandomSearchGrid is too vague and therefor does not improve our model at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the optimal KNN classifier\n",
    "best_params_knn = hyperparameter_tuning(X_train_pop, y_train_pop, KNeighborsClassifier())\n",
    "\n",
    "# Create a new KNN classifier using the best parameters\n",
    "clf_tuned_knn = KNeighborsClassifier(\n",
    "    n_neighbors=best_params_knn['classifier__n_neighbors'],\n",
    "    weights=best_params_knn['classifier__weights'],\n",
    "    algorithm=best_params_knn['classifier__algorithm'],\n",
    "    leaf_size=best_params_knn['classifier__leaf_size'],\n",
    "    p=best_params_knn['classifier__p'])\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_knn_tuned = TfidfVectorizer(ngram_range=best_params_knn['vectorizer__ngram_range'],\n",
    "\t\t\t\t\t\t\t\t\t   max_df=best_params_knn['vectorizer__max_df'], min_df=best_params_knn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned KNN classifier\n",
    "y_pred_clf_tuned_knn, y_score_clf_tuned_knn = evaluate_model(\n",
    "    vectorizer_knn_tuned, clf_tuned_knn, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned KNN classifier's AUC score\n",
    "print('Tuned KNN Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_knn)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 NN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the optimal Neural Net classifier\n",
    "best_params_nn = hyperparameter_tuning(X_train_pop, y_train_pop, MLPClassifier())\n",
    "\n",
    "# Create a new Neural Net classifier using the best parameters\n",
    "clf_tuned_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params_nn['classifier__hidden_layer_sizes'],\n",
    "    activation=best_params_nn['classifier__activation'],\n",
    "    solver=best_params_nn['classifier__solver'],\n",
    "    alpha=best_params_nn['classifier__alpha'],\n",
    "    learning_rate=best_params_nn['classifier__learning_rate'],\n",
    "    max_iter=best_params_nn['classifier__max_iter'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_nn_tuned = TfidfVectorizer(ngram_range=best_params_nn['vectorizer__ngram_range'],\n",
    "                                        max_df=best_params_nn['vectorizer__max_df'], min_df=best_params_nn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned Neural Net classifier\n",
    "y_pred_clf_tuned_nn, y_score_clf_tuned_nn = evaluate_model(\n",
    "    vectorizer_nn_tuned, clf_tuned_nn, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned Neural Net classifier's AUC score\n",
    "print('Tuned Neural Net Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_nn)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Implementing voting classifier\n",
    "\n",
    "# Define a RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Define a MLPClassifier\n",
    "NN = MLPClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('RF', RF), ('NN', NN), ('LogReg', clf_tuned)], voting='hard')\n",
    "\n",
    "y_pred_vote, y_score_vote = evaluate_model(\n",
    "    vectorizer_tuned, voting_clf, X_train_pop, y_train_pop, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier()))\n",
    "level0.append(('NN', MLPClassifier()))\n",
    "level0.append(('LogReg', clf_tuned))\n",
    "\n",
    "# Define meta learner model\n",
    "level1 = clf_tuned\n",
    "\n",
    "# Define the stacking ensemble\n",
    "model_stacking = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "# Evaluate stacked model\n",
    "evaluate_model(vectorizer_tuned, model_stacking, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p pandas,numpy,sklearn,datasets,spacy,wordcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
