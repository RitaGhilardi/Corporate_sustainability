{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/percw/Corporate_sustainability/blob/main/Companies_sustainability.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Github\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <h1>Project Title</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-do (Rita)**: write an introduction of our project and add here the review of the existing solutions and research on greenwashing claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [0 - Imports and Dependencies](#imports-and-dependencies)\n",
    "- [1 - Data Loading](#data-loading)\n",
    "- [2 - Exploratory Data Analysis](#explatory-data-analysis)\n",
    "- [3 - Training, testing and evaluating ML models](#training-testing-models)\n",
    "- [4 - Classifiers](#classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Imports and Dependencies<a id=\"imports-and-dependencies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_url = 'https://raw.githubusercontent.com/percw/Corporate_sustainability/main/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, please make sure you have the required dependencies installed.\n",
    "Requirements.txt file contains all the required dependencies for this notebook.\n",
    "\n",
    "1. If you have forked or dowloaded the repository, you can install the dependencies using the following command:\n",
    "\t```bash\n",
    "\tpip install -r requirements_url\n",
    "\t```\n",
    "\tor using conda:\n",
    "\t```bash\n",
    "\tconda install -r requirements_url\n",
    "\n",
    "\t```\n",
    "\tSplitting imports into three cells to avoid errors.\n",
    "2. If you're using Google Colab, you can run the following cell to install the required dependencies.\n",
    "\n",
    "```bash\n",
    "!pip install -r requirements_url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run the following command to download the english spacy pipeline:\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "if you are using python v.3 or above or above you might need to run the following command:\n",
    "```bash\n",
    "python3 -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations \n",
    "Some of the following code is computation heavy, we therefor recommend to run this code using GoogleColab and doing the following adjustments to the runtime:\n",
    "1. Go to the menu and select \"Runtime\" > \"Change runtime type.\"\n",
    "2. Choose \"GPU\" as the hardware accelerator and click \"Save.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# Importing libraries for data analysis and visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import spacy as spacy\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             HashingVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             confusion_matrix, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV,\n",
    "                                     train_test_split)\n",
    "# Importing libraries for data preprocessing and modeling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob import TextBlob\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Loading<a id=\"data-loading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used to conduct this study is available [at this link](https://huggingface.co/datasets/climatebert/environmental_claims/tree/main). \n",
    "\n",
    "The *Environmental Claims* dataset is a collection textual data that expresses a commitment, initiative, or action made by various entities, such as companies or organizations. Some of these claims concern initiatives towards addressing environmental issues. As such, the dataset includes information on diverse environmental topics, such as renewable energy, carbon footprint reduction, waste management, water conservation, sustainable practices, biodiversity preservation, or eco-friendly products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset('climatebert/environmental_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display to show how the format of the dataset looks like\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each split\n",
    "env_claim_train = pd.DataFrame(dataset['train'])\n",
    "env_claim_test = pd.DataFrame(dataset['test'])\n",
    "env_claim_val = pd.DataFrame(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define color codes\n",
    "class Colors:\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "# Displaying the characteristics of the data\n",
    "print(Colors.OKGREEN + \"Training data\" + Colors.ENDC)\n",
    "print(f\"The shape of the training data is: {env_claim_train.shape}\")\n",
    "display(env_claim_train.head())\n",
    "display(env_claim_train.label.value_counts())\n",
    "\n",
    "print(Colors.OKBLUE + \"\\nTest data\" + Colors.ENDC)\n",
    "print(f\"The shape of the test data is: {env_claim_test.shape}\")\n",
    "display(env_claim_test.head())\n",
    "display(env_claim_test.label.value_counts())\n",
    "\n",
    "print(Colors.OKCYAN + \"\\nValidation data\" + Colors.ENDC)\n",
    "print(f\"The shape of the validation data is: {env_claim_val.shape}\")\n",
    "display(env_claim_val.head())\n",
    "display(env_claim_val.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Environmental Claims* dataset is already divided into three subsets:\n",
    "* **Train set**: it contains 2400 observations, each of them corresponding to a claim. The majority (i.e., 542) of these statements are environmental claims, while 1858 of them are non-environmental statements.\n",
    "\n",
    "\n",
    "* **Test set**: it contains 300 observations, 64 of which are environmental claims.\n",
    "\n",
    "\n",
    "* **Validation set**: it contains 300 observation, 64 of which are environmental claims as for the test set.\n",
    "\n",
    "While the labelled variable will be studied in more detail in the coming sections, we can already note how the dataset seem unbalanced towards non-environmental claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis<a id=\"#explatory-data-analysis\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets \n",
    "claim_dataset = pd.concat([env_claim_train, env_claim_test, env_claim_val], ignore_index = True)\n",
    "print(\"Number of claims in the dataset:\", claim_dataset.shape[0])    # observations\n",
    "print(\"Number of variables in the dataset:\", claim_dataset.shape[1]) # variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs \n",
    "print(\"Number of NaNs:\")\n",
    "display(claim_dataset.isna().sum())\n",
    "\n",
    "# Duplicates\n",
    "print(\"Number of duplicates:\")\n",
    "display(claim_dataset.duplicated().sum())\n",
    "\n",
    "# Variable types\n",
    "print(\"Variable types:\")\n",
    "claim_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count by Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that show the avergage number of words in each claim and a graphical representation by class\n",
    "\n",
    "def word_count_graph(claim_dataset):\n",
    "\t# Word count\n",
    "\tclaim_dataset[\"word count\"] = claim_dataset[\"text\"].apply(lambda x: len(x.split()))\n",
    "\tprint(\"The average number of words in each claim is equal to:\", round(claim_dataset[\"word count\"].mean(),0), \"words.\")\n",
    "\n",
    "\t# Graphical representation by class\n",
    "\tclass_1_counts = claim_dataset[claim_dataset[\"label\"] == 1][\"word count\"]\n",
    "\tclass_2_counts = claim_dataset[claim_dataset[\"label\"] == 0][\"word count\"]\n",
    "\n",
    "\tplt.hist(class_1_counts, bins = range(11, 39), alpha = 0.5, label = \"Environmental Claim\", color = \"#4958B5\")\n",
    "\tplt.hist(class_2_counts, bins = range(11, 39), alpha = 0.5, label = \"Non-environmental Claim\", color = \"#8DB8B7\")\n",
    "\tplt.xlabel(\"Word count\")\n",
    "\tplt.ylabel(\"Frequency\")\n",
    "\tplt.title(\"Number of Words in Each Claim by Class\")\n",
    "\tplt.legend(loc = \"upper right\")\n",
    "\tplt.show()\n",
    "\n",
    "# Applying the function\n",
    "word_count_graph(claim_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the Spacy sp function to each row of the 'text' column\n",
    "claim_dataset[\"spacy object\"] = claim_dataset[\"text\"].apply(sp)\n",
    "\n",
    "# Filter stopwords, punctuation and spaces\n",
    "def filter_tokens(token):\n",
    "    return not token.is_stop and not token.is_punct and not token.is_space\n",
    "\n",
    "# Remove stopwords, punctuation, and whitespace from each Spacy object\n",
    "claim_dataset[\"filtered tokens\"] = claim_dataset[\"spacy object\"].apply(lambda doc: [token.text for token in doc if filter_tokens(token)])\n",
    "\n",
    "print(\"This is the first sentence before filtering:\", claim_dataset.iloc[0,0])\n",
    "print(\"\\nThis is the first sentence after filtering:\", claim_dataset.iloc[0,4])\n",
    "\n",
    "# Calculating new average value of words per claim\n",
    "number_words = [len(x) for x in claim_dataset[\"filtered tokens\"]]\n",
    "print(\"\\nThe average number of words per claim is now:\", round(np.mean(number_words),0))\n",
    "display(claim_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Claims versus Non-Environmental Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean \n",
    "print(\"Average number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").mean().round())\n",
    "\n",
    "# Median\n",
    "print(\"\\nMedian number of words per claim by class:\")\n",
    "display(claim_dataset.groupby(\"label\").median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud hue by class label\n",
    "\n",
    "# Join the strings in each list into a single string\n",
    "claim_dataset[\"joined tokens\"] = claim_dataset[\"filtered tokens\"].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# For Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==1][\"joined tokens\"])\n",
    "wordcloud = WordCloud( background_color = \"white\", colormap = \"Greens\").generate(text)\n",
    "\n",
    "ax1.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "ax1.set(title = \"WordCloud of Environmental Claims\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# For Non-Environmental Claims\n",
    "text = \" \".join(word for word in claim_dataset[claim_dataset[\"label\"]==0][\"joined tokens\"])\n",
    "wordcloud = WordCloud(background_color = \"white\", colormap = \"Reds\").generate(text)\n",
    "\n",
    "ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "ax2.set(title = \"WordCloud of Non-Environmental Claims\")\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the top most frequent words\n",
    "def top_words(claim_dataset, label, n=10):\n",
    "    top = Counter([item for sublist in claim_dataset[\"joined tokens\"]   # sublist is a list of words in each claim\n",
    "                  [claim_dataset[\"label\"] == label] for item in str(sublist).split()])\n",
    "    temp = pd.DataFrame(top.most_common(n))     # Create a dataframe with the top n words\n",
    "    temp.columns = [\"Common Words\", \"Count\"]    # Naming the columns\n",
    "    temp.index = np.arange(1, len(temp) + 1)    # Setting first index to 1\n",
    "    return temp\n",
    "\n",
    "\n",
    "# Top 10 most frequent words for environmental claims\n",
    "print(\"Top 10 most frequent words for environmental claims:\")\n",
    "env_top_words = top_words(claim_dataset, 1)\n",
    "display(env_top_words.style.background_gradient(cmap=\"Greens\"))\n",
    "\n",
    "# Top 10 most frequent words for non-environmental claims\n",
    "print(\"\\nTop 10 most frequent words for non-environmental claims:\")\n",
    "nonenv_top_words = top_words(claim_dataset, 0)\n",
    "display(nonenv_top_words.style.background_gradient(cmap=\"Reds\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Populating with more environmental claims\n",
    "\n",
    "Since are data is heavily underrepresented in class 1, we used ChatGPT 3 and 4 to generate different environmental claims. These can be found in the `env_claims_gpt3.json` and `env_claims_gpt4.json` in the `./data` folder.\n",
    "\n",
    "We will append these statements to the ones we already have, and later investigate whether these can improve our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the env_claims_gpt4.json file from remote\n",
    "claims_gpt4 = []\n",
    "gpt_4_url = 'https://raw.githubusercontent.com/percw/Corporate_sustainability/main/data/env_claims_gpt4.json'\n",
    "gpt_4 = requests.get(gpt_4_url).json()\n",
    "\n",
    "for i in range(len(gpt_4['claims'])):\n",
    "\tclaims_gpt4.append(gpt_4['claims'][i]['claim'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the env_claims_gpt3.json file from remote\n",
    "claims_gpt3 = []\n",
    "gpt_3_url = 'https://raw.githubusercontent.com/percw/Corporate_sustainability/main/data/env_claims_gpt3.json'\n",
    "gpt_3 = requests.get(gpt_3_url).json()\n",
    "\n",
    "for i in range(len(gpt_3['claims'])):\n",
    "\tclaims_gpt3.append(gpt_3['claims'][i]['claim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of claims\n",
    "print(\"Number of claims in the dataset (GPT4):\", len(claims_gpt4))    # observations\n",
    "print(\"Number of claims in the dataset (GPT3):\", len(claims_gpt3))    # observations\n",
    "print(\"Total number of claims in the dataset:\", len(claims_gpt4)+len(claims_gpt3))    # observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list to a dataframe\n",
    "\n",
    "claims_gpt4_df = pd.DataFrame(claims_gpt4, columns = ['text'])\n",
    "claims_gpt3_df = pd.DataFrame(claims_gpt3, columns = ['text'])\n",
    "\n",
    "# Adding the label column\n",
    "claims_gpt4_df['label'] = 1\n",
    "claims_gpt3_df['label'] = 1\n",
    "\n",
    "# Concatenating the two dataframes\n",
    "claims_gpt_df = pd.concat([claims_gpt4_df, claims_gpt3_df], ignore_index=True)\n",
    "display(claims_gpt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we concat the generated data, we can see how the distribution and inbalance has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new df with the original claims and the generated claims\n",
    "claim_pop = pd.concat([claim_dataset, claims_gpt_df], ignore_index=True)\n",
    "\n",
    "# Displaying how the generated has changed the distribution of the dataset\n",
    "print(\"Distribution of the dataset before adding the generated claims:\")\n",
    "display(claim_dataset['label'].value_counts(normalize=True).round(2))\n",
    "\n",
    "print(\"\\nDistribution of the dataset after adding the generated claims:\")\n",
    "display(claim_pop['label'].value_counts(normalize=True).round(2))\n",
    "\n",
    "word_count_graph(claim_pop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Energy Claims using N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Still need to finish this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample of claims with word \"energy\" inside\n",
    "energy_df = claim_dataset[claim_dataset[\"joined tokens\"].str.contains(\"energy\")]\n",
    "print(\"In the original dataset, there are\", len(energy_df), \"claims containing the word 'energy'.\")\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 1\n",
    "energy_df_1 = energy_df[energy_df[\"label\"] == 1]\n",
    "\n",
    "# Subsample of claims with word \"energy\" inside and label == 0\n",
    "energy_df_0 = energy_df[energy_df[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculating most frequent N-Grams given corpus, n-grams\n",
    "\n",
    "def top_n_ngram(energy_corpus, ngram = 3):\n",
    "    vec = CountVectorizer(ngram_range = (ngram,ngram)).fit(energy_corpus)\n",
    "    words_bag = vec.transform(energy_corpus)  # Have the count of  all the words for each claim\n",
    "    sum_words = words_bag.sum(axis = 0)       # Calculates the count of all the word in the whole claim\n",
    "    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n",
    "    return words_freq\n",
    "\n",
    "# Call function on both datasets \n",
    "pop_words_1 = top_n_ngram(energy_df_1[\"joined tokens\"], 3)  \n",
    "pop_words_0 = top_n_ngram(energy_df_0[\"joined tokens\"], 3)  \n",
    "\n",
    "# Select top 20 N-Grams having 'energy' in text \n",
    "pop_energy_1 = [t for t in pop_words_1 if \"energy\" in t[0]]\n",
    "pop_energy_1 = pop_energy_1[:20]\n",
    "pop_energy_0 = [t for t in pop_words_0 if \"energy\" in t[0]]\n",
    "pop_energy_0 = pop_energy_0[:20]\n",
    "\n",
    "# Graphical representation\n",
    "\n",
    "# Extract x and y values from each list\n",
    "x1, y1 = zip(*pop_energy_1)\n",
    "x2, y2 = zip(*pop_energy_0)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 2, figsize = (20,9))\n",
    "fig.subplots_adjust(hspace = 1.2)\n",
    "\n",
    "# Create the first bar plot on ax1\n",
    "ax1.bar(x1, y1, color = \"#4958B5\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xticks(range(len(x1)))\n",
    "ax1.set_xticklabels(x1, rotation = 90)\n",
    "ax1.set_title(\"Top 20 Energy 3-Grams in Environmental Claims\")\n",
    "\n",
    "# Create the second bar plot on ax2\n",
    "ax2.bar(x2, y2, color = \"#8DB8B7\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xticks(range(len(x2)))\n",
    "ax2.set_xticklabels(x2, rotation = 90)\n",
    "ax2.set_title(\"Top 20 Energy 3-Grams in Non-Environmental Claims\")\n",
    "\n",
    "# Add x-axis label\n",
    "fig.add_subplot(111, frameon = False)\n",
    "plt.tick_params(labelcolor = \"none\", top = False, bottom = False, left = False, right = False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the 3-gram lists, it becomes apparent that the terms in Class 1 are more focused on energy generation, efficiency, and reduction of consumption with a clear emphasis on renewable and clean technology. Key phrases like 'improve energy efficiency', 'renewable energy projects', and 'reduce energy consumption' suggest that Class 1 is associated with environmentally proactive actions or strategies.\n",
    "\n",
    "On the other hand, Class 0 appears to be more concerned with the management and infrastructure of energy, including the use of renewable sources, but with notable mentions of 'energy management systems', 'incineration energy recovery', and 'energy storage capacity'. This class seems to focus more on the operational aspects and physical assets related to energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the train and test datasets for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = env_claim_train['text'], env_claim_train['label']\n",
    "X_test, y_test = env_claim_test['text'], env_claim_test['label']\n",
    "X_val, y_val = env_claim_val['text'], env_claim_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Labelled Data and Defining Base Rate\n",
    "\n",
    "TODO: Include gpt generated claims and show graph for both dataframes and label categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class:\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "\n",
    "\n",
    "# Test Set\n",
    "print(Colors.OKBLUE + \"\\nTest set per class:\" + Colors.ENDC)\n",
    "display(y_test.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class:\" + Colors.ENDC)\n",
    "display(y_val.value_counts())\n",
    "\n",
    "\n",
    "# Creating a function that takes in y's returns a a plot of the distribution of the classes\n",
    "def plot_class_distribution(y_list: list, title: str = ''):\n",
    "    ''' \n",
    "    Function that plots the class distribution\n",
    "    Input: List of y vars\n",
    "    Output: Plot of distribution\n",
    "    '''\n",
    "    is_list = isinstance(y_list, list)\n",
    "    if is_list:\n",
    "        outcome_variable = pd.concat(y_list)\n",
    "\n",
    "    else:\n",
    "        outcome_variable = y_list\n",
    "    outcome_variable.value_counts().plot.bar(\n",
    "        color=[\"#4958B5\", \"#8DB8B7\"], grid=False)\n",
    "    plt.ylabel(\"Number of observations\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.title(f\"Number of Observations per Class {title}\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_class_distribution(claim_dataset['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating base rate\n",
    "\n",
    "outcome_variable = claim_dataset['label']\n",
    "base_rate = round(len(outcome_variable[outcome_variable == 0]) / len (outcome_variable), 4)\n",
    "print(f'The base rate is: {base_rate*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the graph above with the populated data from ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting class distribution for the merged populated and given data\n",
    "\n",
    "plot_class_distribution(claim_pop['label'], title=\"(Merged Populated and Given Data)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Labelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Comment: I'm not sure if we need to balance the test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of \"0\" outcomes in the training set\n",
    "train_zeros_idx = pd.Series(y_train[y_train == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "train_zeros_sample_idx = train_zeros_idx.sample(n = sum(y_train == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced training set\n",
    "X_train_bal = pd.concat([X_train[y_train == 1], X_train[train_zeros_sample_idx]])\n",
    "y_train_bal = pd.concat([y_train[y_train == 1], y_train[train_zeros_sample_idx]])\n",
    "\n",
    "\n",
    "# Get indices of \"0\" outcomes in the test set\n",
    "test_zeros_idx = pd.Series(y_test[y_test == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "test_zeros_sample_idx = test_zeros_idx.sample(n = sum(y_test == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced test set\n",
    "X_test_bal = pd.concat([X_test[y_test == 1], X_test[test_zeros_sample_idx]])\n",
    "y_test_bal = pd.concat([y_test[y_test == 1], y_test[test_zeros_sample_idx]])\n",
    "\n",
    "# Get indices of \"0\" outcomes in the validation set\n",
    "val_zeros_idx = pd.Series(y_val[y_val == 0].index)\n",
    "\n",
    "# Randomly select a balanced number of \"0\" outcomes\n",
    "val_zeros_sample_idx = val_zeros_idx.sample(n = sum(y_val == 1), random_state = 7)\n",
    "\n",
    "# Use the sampled indices to get the final balanced validation set\n",
    "X_val_bal = pd.concat([X_val[y_val == 1], X_val[val_zeros_sample_idx]])\n",
    "y_val_bal = pd.concat([y_val[y_val == 1], y_val[val_zeros_sample_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations per class after balancing the classes:\\n\")\n",
    "\n",
    "# Train Set\n",
    "print(Colors.OKGREEN + \"Train set per class\" + Colors.ENDC)\n",
    "display(y_train_bal.value_counts())\n",
    "      \n",
    "\n",
    "# Test Set \n",
    "print(Colors.OKBLUE + \"\\nTest set per class\" + Colors.ENDC)\n",
    "display(y_test_bal.value_counts())\n",
    "\n",
    "# Validation Set\n",
    "print(Colors.OKCYAN + \"\\nValidation set per class\" + Colors.ENDC)\n",
    "display(y_val_bal.value_counts())\n",
    "\n",
    "print(\"\\nThe new balanced dataset contains\", len(y_train_bal + y_test_bal + y_val_bal) , \"observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training, testing and evaluating ML models<a id=\"training-testing-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy\n",
    "\n",
    "We will use the following strategy to train, test and evaluate our models:\n",
    "1. Define different tokenization functions\n",
    "   1. Test different tokenization functions on the Logistic Regression model\n",
    "2. Define different vectorization functions\n",
    "   1. Test different vectorization functions on the Logistic Regression model\n",
    "3. Fine-tune hyperparameters\n",
    "4. Compare the performance of the different pipelines\n",
    "\n",
    "We will in the Classifiers section define different models and compare the performance with the Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression with different tokenization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating several helper functions\n",
    "\n",
    "# Create a spaCy tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def simple_spacy_tokenizer(text):\n",
    "    return [tok.lemma_.lower() for tok in nlp(text) if not tok.is_stop and tok.is_alpha]\n",
    "\n",
    "def spacy_tokenizer_ngrams(text):\n",
    "    # Parse the text with spaCy's language model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Generate n-grams\n",
    "    def generate_ngrams(doc, n):\n",
    "        return [' '.join(doc[i:i+n]) for i in range(len(doc) - n + 1)]\n",
    "\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # Remove stop words and non-alphabetical tokens\n",
    "        if tok.is_alpha and not tok.is_stop:\n",
    "            # Lemmatize and lower case the token\n",
    "            tokens.append(tok.lemma_.lower().strip())\n",
    "\n",
    "        # If the token is a named entity, add it to the list\n",
    "        if tok.ent_type_:\n",
    "            tokens.append(tok.text)\n",
    "\n",
    "    # Add bi-grams to the list of tokens\n",
    "    tokens.extend(generate_ngrams(tokens, 2))\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `simple_spacy_tokenizer` tokenizes the input text, i.e. For each token, it checks if it is an alphabetical character and if it is not a stop word (commonly used words like 'is', 'the', 'and', etc., that do not carry significant meaning on their own). If the token meets these criteria, it is lemmatized, which means it is converted to its base or dictionary form (for example, 'running' becomes 'run'). The function then converts the token to lowercase and strips any leading or trailing white space. The functon `spacey_tokenizer_ngrams` includes bigrams, the function gives a machine learning model a better chance of understanding the text accurately. The final output is a list of processed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_report_prettify(report):\n",
    "    ''' \n",
    "    Creates a df from the precision_recall_fscore_support function\n",
    "    Input: model performance metrics\n",
    "    '''\n",
    "    out_df = pd.DataFrame(report).transpose()\n",
    "    out_df.columns = ['precision', 'recall', 'f1-score', 'support']\n",
    "    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n",
    "    avg_tot.index = [\"avg/total\"]\n",
    "    out_df = pd.concat([out_df, avg_tot])\n",
    "    out_df['support'] = out_df['support'].apply(lambda x: int(x))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_score):\n",
    "    '''\n",
    "    This function plots the ROC curve.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label=\"Random Classifier\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=['0', '1']):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    '''\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=\"Blues\", cbar=False)\n",
    "    \n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [None, 0, None]\n",
    "\n",
    "\n",
    "def evaluate_model(vectorizer, classifier, X_train, y_train, X_test, y_test, with_confusion_matrix=False):\n",
    "    ''' \n",
    "    Function to evaluate the model performance\n",
    "    Input: vectorizer, classifier, X_train, y_train, X_test, y_test\n",
    "    Output: predicted y and y score\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "\n",
    "    # Train the model\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_score = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Adding a title to the dataframe based on the model and vectorizer used and tokenizer\n",
    "    title = str(classifier).split(\n",
    "        '(')[0] + ' with ' + str(vectorizer).split('(')[0]\n",
    "    print(\"-\" * len(title))\n",
    "    print(Colors.OKGREEN + title + Colors.ENDC)\n",
    "    print(\"-\" * len(title))\n",
    "\n",
    "    # Calculate accuracy and print the performance metrics\n",
    "    performance_df = classification_report_prettify(\n",
    "        precision_recall_fscore_support(y_test, y_pred))\n",
    "    display(performance_df)\n",
    "    # Accuracy is the number of correct predictions divided by the total number of predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Print the accuracy with 3 decimal points\n",
    "    print(Colors.OKBLUE + f'Accuracy: {accuracy*100:0.2f}%' + Colors.ENDC)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if with_confusion_matrix:\n",
    "        plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Updating the best model\n",
    "    global best_model\n",
    "    if accuracy > best_model[1]:\n",
    "        best_model = [pipe, accuracy, performance_df]\n",
    "        print(Colors.OKGREEN + \"\\nBest model updated!\" + Colors.ENDC)\n",
    "    else:\n",
    "        print(Colors.OKBLUE + \"\\nBest model not updated!\" + Colors.ENDC)\n",
    "\n",
    "    # Return the predicted y's and y score and the performance dataframe\n",
    "    return y_pred, y_score, performance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call create a vectorizer and a logistic regression model and evaluate it on the functions created above with the two different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc_curves(names, y_scores, title=\"\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot the ROC curve for each vectorizer/tokenizer/model combination\n",
    "    for name, y_score in zip(names, y_scores):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=name+(' (area = %0.3f)' % roc_auc))\n",
    "\n",
    "    # Plot the ROC curve of a purely random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "    # Plot the ROC curve of a perfect classifier\n",
    "    plt.plot([0, 0, 1], [0, 1, 1], 'k:', label='Perfect Classifier')\n",
    "\n",
    "    # Add labels and legend to the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves {title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "tokenizer_list = [simple_spacy_tokenizer,\n",
    "                  spacy_tokenizer_ngrams]   # Tokenizer list\n",
    "\n",
    "tokenizer_y_score = []  # List to store the y_score for each tokenizer\n",
    "\n",
    "# Going through list of tokenizers and evaluating the model\n",
    "for tokenizer in tokenizer_list:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenizer, ngram_range=(1, 2), max_df=0.85, min_df=2)\n",
    "    y_pred, y_score, df = evaluate_model(\n",
    "        vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "    tokenizer_y_score.append(y_score)    # Append y_score to the list\n",
    "\n",
    "# Plotting the ROC curve for each tokenizer\n",
    "plot_roc_curves([\"Simple Spacy Tokenizer\", \"Spacy Tokenizer\",\n",
    "                \"NLTK Tokenizer\"], tokenizer_y_score, 'Logistic Regression with different tokenizers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model with a bit more advanced tokenizer has a slightly higher accuracy but the same ROC-AUC score. This means that the second tokenizer is slightly better at correctly classifying instances overall, but both models are almost equally good at distinguishing between the classes, as indicated by the ROC-AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression with different vectorization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for Word2Vec to allow it to be used in a scikit-learn Pipeline\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [doc.split() for doc in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.size, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.model.wv[w] for w in doc.split() if w in self.model.wv]\n",
    "                                or [np.zeros(self.size)], axis=0) for doc in X])\n",
    "\n",
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer_ngrams)\n",
    "y_pred_c_vec, y_score_c_vec, c_vec_perf_df = evaluate_model(count_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer(tokenizer=spacy_tokenizer_ngrams)\n",
    "y_pred_h_vec, y_score_h_vec, h_vec_perf_df = evaluate_model(hashing_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Word2Vec\n",
    "w2v_vectorizer = Word2VecVectorizer()\n",
    "y_pred_w2v, y_score_w2v, w2v_perf_df = evaluate_model(w2v_vectorizer, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer_ngrams, ngram_range=(1,2), max_df=0.85, min_df=2)\n",
    "y_pred_tfidf, y_score_tfidf, tfidf_perf_df = evaluate_model(tfidf_vectorizer, clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve for each vectorizer\n",
    "plot_roc_curves([\"Count Vectorizer\", \"Hashing Vectorizer\", \"Word2Vec Vectorizer\", 'TF-IDF Vectorizer'],\n",
    "\t\t\t\t[y_score_c_vec, y_score_h_vec, y_score_w2v, y_score_tfidf], 'Logistic Regression with different Vectorizers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the TF-IDF, CountVectorizer and HashingVectorizer perform equally well. We will fine-tune the hyperparameters of the TF-IDF vectorizer and use it in the next section. We see that the Word2Vec Vectorizer performs poorly which might be due to the small size of our dataset. We can see if it improves with the validation set, augemented data, and populated data from ChatGPT later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TF-IDF Vectorization and Logistic Regression Fine Tuning Hyperparameters\n",
    "\n",
    "In the initial model, we used a TF-IDF Vectorizer and a Logistic Regression classifier to predict whether a statement is an environmental claim or not. However, the model's performance can often be improved by tuning the hyperparameters of the vectorizer and the classifier.\n",
    "\n",
    "Hyperparameters are parameters that are not learned from the data. They are set prior to the commencement of the learning process. For instance, in the case of TF-IDF Vectorizer, `ngram_range`, `max_df`, and `min_df` are hyperparameters. For the Logistic Regression classifier, `C`, which is the inverse of regularization strength, is a hyperparameter. \n",
    "\n",
    "Hyperparameter tuning involves selecting the combination of hyperparameters for a machine learning model that performs the best on a validation set.\n",
    "\n",
    "#### Steps for Hyperparameter Tuning\n",
    "\n",
    "1. **Pipeline Creation**: We first created a pipeline that combines the vectorizer and the classifier. This allows us to jointly optimize the hyperparameters of both.\n",
    "\n",
    "2. **Define Hyperparameters**: We then defined a list of hyperparameters to tune for both the vectorizer and the classifier. For the vectorizer, we decided to tune `ngram_range`, `max_df`, and `min_df`. For the classifier, we decided to tune `C`.\n",
    "\n",
    "3. **Grid Search**: Next, we performed a grid search to find the combination of hyperparameters that results in the best cross-validated performance on the training data. Grid search works by training and evaluating a model for each combination of hyperparameters, and selecting the combination that performs best.\n",
    "\n",
    "4. **Best Parameters**: After the grid search, we printed the combination of hyperparameters that performed the best.\n",
    "\n",
    "5. **Evaluate the Model**: Finally, we used the best hyperparameters to create a new vectorizer and classifier, and evaluated the performance of the model on the test data.\n",
    "\n",
    "This process allowed us to optimize the model's performance by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new hyperparameter tuning function that doesn't include the tokenizer\n",
    "def hyperparameter_tuning(X_train, y_train, classifier):\n",
    "    ''' \n",
    "    Function to perform hyperparameter tuning for different classifiers\n",
    "    Input: X_train, y_train\n",
    "    Output: best_params_\n",
    "    '''\n",
    "    # Create a pipeline with the vectorizer and classifier\n",
    "    pipe = Pipeline([('vectorizer', TfidfVectorizer()), \n",
    "                     ('classifier', classifier)])\n",
    "    \n",
    "    # Define the hyperparameters to tune for LogisticRegression\n",
    "    if isinstance(classifier, LogisticRegression):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "        }\n",
    "         # Perform grid search\n",
    "        grid_search = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "        return grid_search.best_params_\n",
    "\n",
    "    # Define the hyperparameters to tune for RandomForest\n",
    "    elif isinstance(classifier, RandomForestClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters to tune for KNeighborsClassifier\n",
    "    elif isinstance(classifier, KNeighborsClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "            'classifier__weights': ['uniform', 'distance'],\n",
    "            'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "            'classifier__p': [1, 2]\n",
    "        }\n",
    "        # Perform grid search\n",
    "        random_search = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    # Define the hyperparameters for NN\n",
    "    elif isinstance(classifier, MLPClassifier):\n",
    "        params = {\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__max_df': [0.85, 0.9, 0.95],\n",
    "            'vectorizer__min_df': [1, 2, 3],\n",
    "            'classifier__hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "            'classifier__activation': ['tanh', 'relu'],\n",
    "            'classifier__solver': ['sgd', 'adam'],\n",
    "            'classifier__alpha': [0.0001, 0.05],\n",
    "            'classifier__learning_rate': ['constant','adaptive'],\n",
    "        }\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(pipe, param_distributions=params, n_iter=100, cv=5, verbose=1, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        return random_search.best_params_\n",
    " \n",
    "# Preprocess the data using the spaCy tokenizer\n",
    "X_train_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_train] \n",
    "X_test_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_test]   \n",
    "\n",
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_tokenized, y_train, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "# Evaluate the model using previously made function\n",
    "y_pred_clf_tfidf_tuned, y_score_clf_tfidf_tuned, clf_tfidf_tuned_perf_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_tokenized, y_train, X_test_tokenized, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy improves with a little over 1% by fine-tuning the hyperparameters. But we still se that the simple CountVectorizer performed slightly better. We will see if including more data changes this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TF-IDF Vectorization and Logistic Regression with Balanced Data\n",
    "\n",
    "We saw previously that our data was heavily skewed, now let's try to run our logistic model with the balanced data created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_bal, y_train_bal, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_bal_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_bal_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "# Evaluate the model using previously made function\n",
    "y_pred_clf_tfidf_bal, y_score_clf_tfidf_bal, clf_tfidf_bal_perf = evaluate_model(vectorizer_bal_tuned, clf_bal_tuned, X_train_bal, y_train_bal, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the decrease in the training data evidently led to decrease in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Text Classification using TF-IDF Vectorization and Logistic Regression with Generated Claims\n",
    "With populated data from ChatGPT3&4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chatgpt_train, y_chatgpt_train = claims_gpt_df['text'], claims_gpt_df['label']\n",
    "\n",
    "# Concatenate the train, validation and env_claims sets\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X_train_pop = pd.concat([X_train, X_chatgpt_train])\n",
    "y_train_pop = pd.concat([y_train, y_chatgpt_train])\n",
    "X_train_val_pop = pd.concat([X_train, X_val, X_chatgpt_train])\n",
    "y_train_val_pop = pd.concat([y_train, y_val, y_chatgpt_train])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set per class\" + Colors.ENDC)\n",
    "display(y_train.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with populated claims\" + Colors.ENDC)\n",
    "display(y_train_pop.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation claims\" + Colors.ENDC)\n",
    "display(y_train_val.value_counts())\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with validation and populated claims\" + Colors.ENDC)\n",
    "display(y_train_val_pop.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Trying our best LGR model with the additional validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using the spaCy tokenizer\n",
    "X_train_val_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_train_val]  \n",
    "\n",
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params = hyperparameter_tuning(X_train_val, y_train_val, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_val_tuned = TfidfVectorizer(ngram_range=best_params['vectorizer__ngram_range'], \n",
    "                             max_df=best_params['vectorizer__max_df'], min_df=best_params['vectorizer__min_df'])\n",
    "\n",
    "clf_val_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "# Evaluating the performance of the validated data\n",
    "y_pred_clf_tfidf_val, y_score_clf_tfidf_val, clf_tfidf_val_perf_df = evaluate_model(vectorizer_val_tuned, clf_val_tuned, X_train_val_tokenized, y_train_val, X_test_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't seem to improve our model at all. Still standing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Trying our best LGR model with the additional populated data (from ChatGPT3&4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using the spaCy tokenizer\n",
    "X_train_pop_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_train_pop]  \n",
    "\n",
    "# Evaluating the performance of the populated data\n",
    "y_pred_clf_tfidf_pop, y_score_clf_tfidf_pop, clf_tfidf_pop_perf_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_pop_tokenized, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the validated data\n",
    "y_pred_clf_tfidf_val, y_score_clf_tfidf_val, clf_tfidf_val_perf_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the CountVectorizer performs better with the populated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using the spaCy tokenizer\n",
    "X_train_pop_tokenized = [' '.join(spacy_tokenizer_ngrams(text)) for text in X_train_pop]  \n",
    "\n",
    "# Perform hyperparameter tuning on the tokenized data\n",
    "best_params_pop = hyperparameter_tuning(X_train_val, y_train_val, LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "count_vectorizer_tuned = CountVectorizer(ngram_range=best_params_pop['vectorizer__ngram_range'], \n",
    "                             max_df=best_params_pop['vectorizer__max_df'], min_df=best_params_pop['vectorizer__min_df'])\n",
    "\n",
    "clf_pop_tuned = LogisticRegression(solver='liblinear', C=best_params['classifier__C'])\n",
    "\n",
    "\n",
    "y_pre_clf_count_pop, y_score_clf_count_pop, clf_count_pop_perf_df = evaluate_model(count_vectorizer_tuned, clf_tuned, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the more sophisticated TF-IDF Vectorizer performs better than the simple CountVectorizer when we used the populated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! it actually increased quite a lot! Cool, let's try to use them both and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Trying our best LGR model with the additional populated and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating the performance of the populated and validated data\n",
    "y_pred_clf_tfidf_val_pop, y_score_clf_tfidf_val_pop, clf_tfidf_val_pop_per_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_val_pop, y_train_val_pop, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the additional training-data from ChatGPT and the validation data gave the best performance. We used different prompts, asking for info from websites, annual reports and so on, in addition to differ in length (word count). With both datasets, we can see that the model is able to generalize better and perform better on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Trying our best LGR model with populated (ChatGPT3&4) and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(sentences, labels, augment_times=1):\n",
    "    '''\n",
    "    Function to perform data augmentation\n",
    "    Input: sentences - list of sentences\n",
    "           labels - list of labels corresponding to the sentences\n",
    "           augment_times - number of times to augment each sentence\n",
    "    Output: aug_sentences - list of augmented sentences\n",
    "            aug_labels - list of labels for the augmented sentences\n",
    "    '''\n",
    "    synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "    aug_sentences = []\n",
    "    aug_labels = []\n",
    "\n",
    "    for i in range(augment_times):\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            # Apply synonym augmentation\n",
    "            new_sentence = synonym_aug.augment(sentence)\n",
    "\n",
    "            # Only append the new sentence if it's not NaN\n",
    "            if pd.notnull(new_sentence):\n",
    "                aug_sentences.append(new_sentence)\n",
    "                aug_labels.append(label)\n",
    "\n",
    "    return aug_sentences, aug_labels\n",
    "\n",
    "# Only augmenting the label 1 sentences\n",
    "X_train_aug_1, y_train_aug_1 = data_augmentation(X_train[y_train == 1].values.tolist(), y_train[y_train == 1].values.tolist())\n",
    "\n",
    "# Convert the list of augmented sentences and labels to pandas Series\n",
    "X_train_aug_1 = pd.Series(X_train_aug_1, index = range(len(X_train), len(X_train) + len(X_train_aug_1)))\n",
    "y_train_aug_1 = pd.Series(y_train_aug_1, index = range(len(X_train), len(X_train) + len(y_train_aug_1)))\n",
    "\n",
    "# Feeding the data_augmentation function with the training data\n",
    "augmented_sentences, augmented_labels = data_augmentation(X_train.values.tolist(), y_train.values.tolist())\n",
    "\n",
    "# Convert the list of augmented sentences and labels to pandas Series\n",
    "augmented_sentences = pd.Series(augmented_sentences, index = range(len(X_train), len(X_train) + len(augmented_sentences)))\n",
    "augmented_labels = pd.Series(augmented_labels, index = range(len(X_train), len(X_train) + len(augmented_labels)))\n",
    "\n",
    "# Concatenate the augmented sentences and labels with the ChatGPT Populated and original training data\n",
    "X_train_aug = pd.concat([X_train_pop, augmented_sentences])\n",
    "y_train_aug = pd.concat([y_train_pop, augmented_labels])\n",
    "\n",
    "# Print the number of observations per class\n",
    "print(Colors.OKBLUE + \"\\nTrain set with augmented data\" + Colors.ENDC)\n",
    "display(y_train_aug.value_counts())\n",
    "\n",
    "# Convert list of words in each document into a single string\n",
    "X_train_aug = [\" \".join(sublist) for sublist in X_train_aug]\n",
    "\n",
    "# Now we can evaluate the model\n",
    "y_pred_clf_tfidf_aug, y_score_clf_tfidf_aug, clf_tfidf_aug_per_df = evaluate_model(vectorizer_tuned, clf_tuned, X_train_aug, y_train_aug, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_data_input = [y_score_clf_tfidf_tuned, y_score_clf_tfidf_bal,\n",
    "                       y_score_clf_tfidf_val, y_score_clf_tfidf_pop, y_score_clf_tfidf_aug, y_score_clf_tfidf_val_pop]\n",
    "data_input_names = ['BenchMark', 'Balanced', 'Validation',\n",
    "                    'ChatGPT', 'ChatGPT + Aug', 'ChatGPT + Val']\n",
    "\n",
    "plot_roc_curves(data_input_names, y_scores_data_input,\n",
    "                'TF-IDF Logistic Regression for different Data Input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. By comparing different ROC curves, we can make informed decisions about which vectorizer is best suited for our specific task based on its performance in terms of trade-off between sensitivity (TPR) and specificity (1 - FPR).\n",
    "\n",
    "Each line in the plot corresponds to a different data input. The closer a curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. This means the top left corner of the plot is the 'ideal' point - a false positive rate of zero, and a true positive rate of one. Therefore, a model whose ROC curve is closer to the top left corner performs better than a model whose curve is closer to the diagonal line.\n",
    "\n",
    "The diagonal line in the middle of the plot represents a random classifier (e.g., a coin flip), which has an equal chance of giving a correct or incorrect classification. Any good classifier should have its ROC curve above this line. If a curve is below this line, it means the classifier is worse than random chance. \n",
    "\n",
    "The dotted line represents a perfect classifier.\n",
    "\n",
    "Interestingly for the ChatGPT+Augmented data curve goes to 1 on the True Positive Rate scale before reaching 0.7 on the False Positive Rate, which means that our model is able to achieve a high rate of true positives (correctly identified positive instances) with a relatively low rate of false positives (negative instances incorrectly identified as positive). \n",
    "\n",
    "But when looking at other performance metrics such as accuracy, precision and recall, and area under curve in total, the ChatGPT data performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot the ROC as a bar chart\n",
    "def plot_roc_bars(names, y_scores, title):\n",
    "    '''\n",
    "    Function to plot the ROC curves as a bar chart\n",
    "    Input: names - list of names for the ROC curves\n",
    "           y_scores - list of y_scores for the ROC curves\n",
    "           title - title of the plot\n",
    "    Output: Plot of ROC curves\n",
    "    '''\n",
    "    # Create a list of vectorizer AUC scores\n",
    "    auc_scores = [roc_auc_score(y_test, y_score)\n",
    "                  for y_score in y_scores]\n",
    "\n",
    "    # Create a DataFrame of vectorizer performance\n",
    "    performance = pd.DataFrame(\n",
    "        {'Data': names, 'AUC Score': auc_scores})\n",
    "\n",
    "    # Sort the DataFrame by AUC score\n",
    "    performance.sort_values(\n",
    "        by='AUC Score', ascending=True, inplace=True, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='Data', y='AUC Score', data=performance,\n",
    "                palette='Blues')  # AUC = Area Under the Curve (ROC)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plotting the AUC scores on each bar\n",
    "    for index, row in performance.iterrows():\n",
    "        plt.text(index, row['AUC Score'] + 0.005,\n",
    "                 round(row['AUC Score'], 3), ha='center', color='black')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plotting the ROC curves for the different data inputs\n",
    "plot_roc_bars(data_input_names, y_scores_data_input,\n",
    "                \"TF-IDF Logistic Regression Model Performance with various Data Input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart provided offers a visual comparison of the performance of different vectorizers used in our text classification model, as measured by the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. AUC-ROC is a valuable metric for this purpose, as it provides a comprehensive view of model performance across all possible classification thresholds, unlike accuracy, precision, or recall which depend on a specific threshold. An AUC-ROC score close to 1.0 indicates that the model has a high ability to distinguish between the classes correctly, regardless of the threshold chosen. Therefore, in this graph, the TF-IDF vectorizer is the one that, on average, best discriminates between the classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import precison_recall_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Creating a functin that plots the relationship between the precision and the recall of several models\n",
    "def plot_precision_recall_curve(model_names, y_scores, title):\n",
    "\t'''\n",
    "\tFunction to plot the precision recall curve\n",
    "\tInput: model_names - list of model names\n",
    "\t\t   y_scores - list of y_scores for each model\n",
    "\t\t   title - title of the plot\n",
    "\tOutput: None\n",
    "\t'''\n",
    "\tplt.figure(figsize=(8, 6))\n",
    "\tfor model_name, y_score in zip(model_names, y_scores):\n",
    "\t\tprecision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "\t\tplt.plot(recall, precision, label=model_name)\n",
    "\n",
    "\t# Plotting the baseline\n",
    "\tplt.plot([0, 1], [0.5, 0.5], linestyle='--', label='Baseline')\n",
    "\n",
    "\tplt.xlabel('Recall')\n",
    "\tplt.ylabel('Precision')\n",
    "\tplt.title(title)\n",
    "\tplt.legend()\n",
    "\tplt.xlim(0, 1)\n",
    "\tplt.ylim(0, 1)\n",
    "\tplt.show()\n",
    "\n",
    "# Plotting the precision recall curve for the different data inputs\n",
    "plot_precision_recall_curve(data_input_names, y_scores_data_input, 'TF-IDF Logistic Regression Precision Recall Curve for different Data Input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Classifiers<a id=\"classifiers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers including KNN, MLPC, LogReg, MultinomialNB, RandomForest\n",
    "classifiers = [KNeighborsClassifier(), MLPClassifier(),\n",
    "               clf_tuned, MultinomialNB(), RandomForestClassifier()]\n",
    "\n",
    "vectorizers = [tfidf_vectorizer, w2v_vectorizer,\n",
    "               tfidf_vectorizer, count_vectorizer, tfidf_vectorizer]\n",
    "\n",
    "# Create a list of classifier names containing the names of the classifiers with the most suitable vectorizer\n",
    "classifier_names = ['KNN', 'Neural Net',\n",
    "                    'Logistic Regression', 'Naive Bayes', 'Random Forest']\n",
    "\n",
    "# Create a json to store the classifiers and vectorizers with the most suitable vectorizer\n",
    "classifiers_dict = dict(zip(classifier_names, classifiers))\n",
    "# Assuming 'vectorizers' is your list of vectorizers\n",
    "vectorizers_dict = dict(zip(classifier_names, vectorizers))\n",
    "\n",
    "# Create a list of classifier predictions\n",
    "y_preds = []\n",
    "y_scores = []\n",
    "\n",
    "# Feeding all the prediction into the list of classifiers\n",
    "for classifier_name in classifiers_dict:\n",
    "    y_pred, y_score, df = evaluate_model(\n",
    "        vectorizers_dict[classifier_name], classifiers_dict[classifier_name], X_train, y_train, X_test, y_test)\n",
    "\n",
    "    y_preds.append(y_pred)\n",
    "    y_scores.append(y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the ROC curves for the different classifiers\n",
    "plot_roc_curves(classifier_names, y_scores, \"Different Classifiers ROC Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_roc_bars(classifier_names, y_scores, \"Different Classifiers ROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest seems promising to further develop and tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fine Tuning the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_params_rf = hyperparameter_tuning(X_train, y_train, RandomForestClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest classifier using the best parameters\n",
    "clf_tuned_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params_rf['classifier__n_estimators'],\n",
    "    max_depth=best_params_rf['classifier__max_depth'],\n",
    "    min_samples_split=best_params_rf['classifier__min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['classifier__min_samples_leaf'],\n",
    "    bootstrap=best_params_rf['classifier__bootstrap'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_rf_tuned = TfidfVectorizer(ngram_range=best_params_rf['vectorizer__ngram_range'], \n",
    "                             max_df=best_params_rf['vectorizer__max_df'], min_df=best_params_rf['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned random forest classifier\n",
    "y_pred_clf_tuned_rf, y_score_clf_tuned_rf, rf_tuned_df = evaluate_model(\n",
    "    vectorizer_rf_tuned, clf_tuned_rf, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the RandomSearchGrid is too vague and therefor does not improve our model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Creating the optimal KNN classifier\n",
    "best_params_knn = hyperparameter_tuning(\n",
    "    X_train_val_pop, y_train_val_pop, KNeighborsClassifier())\n",
    "\n",
    "# Create a new KNN classifier using the best parameters\n",
    "clf_tuned_knn = KNeighborsClassifier(\n",
    "    n_neighbors=best_params_knn['classifier__n_neighbors'],\n",
    "    weights=best_params_knn['classifier__weights'],\n",
    "    algorithm=best_params_knn['classifier__algorithm'],\n",
    "    leaf_size=best_params_knn['classifier__leaf_size'],\n",
    "    p=best_params_knn['classifier__p'])\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_knn_tuned = TfidfVectorizer(ngram_range=best_params_knn['vectorizer__ngram_range'],\n",
    "                                       max_df=best_params_knn['vectorizer__max_df'], min_df=best_params_knn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned KNN classifier\n",
    "y_pred_clf_tuned_knn, y_score_clf_tuned_knn, knn_tuned_df = evaluate_model(\n",
    "    vectorizer_knn_tuned, clf_tuned_knn,  X_train_val_pop,  y_train_val_pop, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 NN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Creating the optimal Neural Net classifier\n",
    "best_params_nn = hyperparameter_tuning(X_train_pop, y_train_pop, MLPClassifier())\n",
    "\n",
    "# Create a new Neural Net classifier using the best parameters\n",
    "clf_tuned_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params_nn['classifier__hidden_layer_sizes'],\n",
    "    activation=best_params_nn['classifier__activation'],\n",
    "    solver=best_params_nn['classifier__solver'],\n",
    "    alpha=best_params_nn['classifier__alpha'],\n",
    "    learning_rate=best_params_nn['classifier__learning_rate'],\n",
    "    max_iter=best_params_nn['classifier__max_iter'],\n",
    "    random_state=42)\n",
    "\n",
    "# Using the best parameters to create our vectorizer and classifier\n",
    "vectorizer_nn_tuned = TfidfVectorizer(ngram_range=best_params_nn['vectorizer__ngram_range'],\n",
    "                                        max_df=best_params_nn['vectorizer__max_df'], min_df=best_params_nn['vectorizer__min_df'])\n",
    "\n",
    "# Evaluate the performance of the tuned Neural Net classifier\n",
    "y_pred_clf_tuned_nn, y_score_clf_tuned_nn, tuned_nn_df = evaluate_model(\n",
    "    vectorizer_nn_tuned, clf_tuned_nn, X_train_pop, y_train_pop, X_test, y_test)\n",
    "\n",
    "# Print the tuned Neural Net classifier's AUC score\n",
    "print('Tuned Neural Net Classifier AUC Score: {:.2f}'.format(\n",
    "    roc_auc_score(y_test, y_score_clf_tuned_nn)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Implementing voting classifier\n",
    "\n",
    "# Define a RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Define a MLPClassifier\n",
    "NN = MLPClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('RF', RF), ('NN', NN), ('LogReg', clf_tuned)], voting='hard')\n",
    "\n",
    "y_pred_vote, y_score_vote = evaluate_model(\n",
    "    vectorizer_tuned, voting_clf, X_train_pop, y_train_pop, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier()))\n",
    "level0.append(('NN', MLPClassifier()))\n",
    "level0.append(('LogReg', clf_tuned))\n",
    "\n",
    "# Define meta learner model\n",
    "level1 = clf_tuned\n",
    "\n",
    "# Define the stacking ensemble\n",
    "model_stacking = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "# Evaluate stacked model\n",
    "evaluate_model(vectorizer_tuned, model_stacking, X_train_pop, y_train_pop, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- BERT Model<a id=\"bert\"></a>\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained model developed by Google. It has been widely adopted for many Natural Language Processing (NLP) tasks due to its great performance. BERT is trained on a large chunks of text and hence, has learned a rich understanding of language, including context, semantics, and grammar. These qualities make it a great choice for our task. Let's see how it performs on our environmental claim dataset.\n",
    "\n",
    "Before using the heavyweight full BERT mode, we can use a lightweight version: **DistilBert** model instead. DistilBert is a smaller, faster, and lighter version of Bert that retains 95% of Bert’s performance while being 60% smaller and 2.5 times faster.\n",
    "\n",
    "Step-by-step Breakdown\n",
    "1. **Import necessary libraries**: Specifically for this task we need PyTorch and Transformers.\n",
    "2. **Data Loading**: We load the training, validation, and testing data using Pandas. This data is provided in CSV files. Each line in the file represents a text and its corresponding label (1 for climate change claim, 0 for no climate change claim).\n",
    "3. **Tokenization**: Next, we use the BERT tokenizer to convert the text into tokens that the BERT model can understand. We pad and truncate all sentences to a single constant length.\n",
    "4. **Dataset and DataLoader**: We create a PyTorch Dataset from the tokenized data. This allows us to use a DataLoader, which makes it easy to efficiently feed data in batches to the neural network.\n",
    "5. **BERT Model**: We load a pre-trained BERT model for sequence classification from the Hugging Face library. This model is designed to handle tasks like ours, where we have a sequence of tokens as input and a single label as output.\n",
    "6. **Optimizer**: We choose the AdamW optimizer with a learning rate of 1e-5. This optimizer is known to work well with BERT.\n",
    "7. **Training Loop**: We train the BERT model for a specified number of epochs. In each epoch, we iterate over batches of data, feed them to the model, and update the model's parameters based on the computed gradients.\n",
    "8. **Evaluation:** Finally, after training, we evaluate the model on the validation set. We calculate the model's predictions and compare them with the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Light Weight BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed for the BERT model\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading data\n",
    "train_df = env_claim_train\n",
    "test_df = env_claim_test\n",
    "val_df = env_claim_val\n",
    "\n",
    "# Load DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create a data loader\n",
    "train_dataset = ClimateDataset(train_encodings, list(train_df['label']))\n",
    "val_dataset = ClimateDataset(val_encodings, list(val_df['label']))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load DistilBert model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) \n",
    "num_epochs = 2\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "    predictions.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=['No Climate Claim', 'Climate Claim']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experienced that GoogleColab where effective at running the code so we try to use the heavyweight BERT model with more epochs. This might longer to run, but hopefully we can get a better performance.\n",
    "\n",
    "### 6.1 BERT Model with 3 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading data\n",
    "train_df = env_claim_train\n",
    "test_df = env_claim_test\n",
    "val_df = env_claim_val\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True)\n",
    "\n",
    "# Creating the ClimateDataset class to load the data into PyTorch\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create a data loader\n",
    "train_dataset = ClimateDataset(train_encodings, list(train_df['label']))\n",
    "val_dataset = ClimateDataset(val_encodings, list(val_df['label']))\n",
    "test_dataset = ClimateDataset(test_encodings, list(test_df['label']))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train loss: {train_loss}, Val loss: {val_loss}')\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "    predictions.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=['No Climate Claim', 'Climate Claim']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Optimizing BERT and using the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading data\n",
    "train_df = env_claim_train\n",
    "test_df = env_claim_test\n",
    "val_df = env_claim_val\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True)\n",
    "\n",
    "# Creating the ClimateDataset class to load the data into PyTorch\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create a data loader\n",
    "train_dataset = ClimateDataset(train_encodings, list(train_df['label']))\n",
    "val_dataset = ClimateDataset(val_encodings, list(val_df['label']))\n",
    "test_dataset = ClimateDataset(test_encodings, list(test_df['label']))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train loss: {train_loss}, Val loss: {val_loss}')\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "    predictions.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=['No Climate Claim', 'Climate Claim']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wee can see that the accuracy is now at 91% which is quite impressive. It will be interesting to see what BERT's environmental claim pre-trained  model will do.\n",
    "\n",
    "### 6.3 BERT Model with Environmental Claim Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \n",
    "    \"climatebert/distilroberta-base-climate-detector\", \n",
    "    num_labels=2, \n",
    "    args={\"reprocess_input_data\": True, \"overwrite_output_dir\": True},\n",
    "    use_cuda=use_cuda\n",
    ")\n",
    "\n",
    "# Train the model on our dataset\n",
    "model.train_model(env_claim_train)\n",
    "\n",
    "# Evaluation\n",
    "def get_predictions(texts, true_labels=None):\n",
    "    predictions, raw_outputs = model.predict(texts)\n",
    "\n",
    "    if true_labels is not None:\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "texts = list(env_claim_test['text'])\n",
    "true_labels = list(env_claim_test['label'])\n",
    "predicted_labels = get_predictions(texts, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a 87% accuracy, which is not as good as our previous best at 91%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model citation:** \n",
    "```bibtex\n",
    "Bingler, J., Kraus, M., Leippold, M., & Webersinke, N. (2023). How Cheap Talk in Climate Disclosures Relates to Climate Initiatives, Corporate Emissions, and Reputation Risk. *Working paper*. Available at SSRN 3998435.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- Fine Tuning ChatGPT-3 Model<a id=\"chatgpt\"></a>\n",
    "\n",
    "Code below is so far just an outline and has not been tested yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def create_prompt_json(claims, labels):\n",
    "    json_list = []\n",
    "    for index, claim in enumerate(claims):\n",
    "        if labels[index] == 0:\n",
    "            json_list.append(\n",
    "                {\"prompt\": f\"{claim} ->\", \"completion\": \"Non-environmental claim.\\n\"})\n",
    "        else:\n",
    "            json_list.append(\n",
    "                {\"prompt\": f\"{claim} ->\", \"completion\": \"Environmental claim.\\n\"})\n",
    "    return json_list\n",
    "\n",
    "# Create function that that creates a json file for each claim in the test set\n",
    "\n",
    "\n",
    "def create_json_files(claims, labels):\n",
    "    json_list = create_prompt_json(claims, labels)\n",
    "    for index, json_data in enumerate(json_list):\n",
    "        with open(f'prompt-data/{index}.json', 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "\n",
    "\n",
    "claims = list(env_claim_test['text'])\n",
    "labels = list(env_claim_test['label'])\n",
    "create_json_files(claims, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "\n",
    "# 1. OpenAI Key\n",
    "api_key =\"YOUR_OPENAI_API_KEY\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "# 2. Create Training Data - this is an example, replace it with your data processing\n",
    "# Todo: convert our data to the following format\n",
    "data_file = [{\n",
    "    \"prompt\": \"Environmental claim ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"Environmental claim ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "}]\n",
    "\n",
    "# 3. Save dict as JSONL file\n",
    "file_name = \"training_data.jsonl\"\n",
    "with open(file_name, 'w') as outfile:\n",
    "    for entry in data_file:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "\n",
    "!openai tools fine_tunes.prepare_data -f training_data.jsonl\n",
    "\n",
    "# 4. Upload file to your OpenAI account\n",
    "upload_response = openai.File.create(\n",
    "  file=file_name,\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "\n",
    "# Save file ID\n",
    "file_id = upload_response.id\n",
    "\n",
    "# 5. Fine-tune a model\n",
    "model=\"gpt-3.5-turbo\"  # Use the model of your choice (e.g. ada, babbage, curie, davinci, etc.)\n",
    "fine_tune_response = openai.FineTune.create(training_file=file_id, model=model)\n",
    "\n",
    "# 7. Save the fine-tuned model\n",
    "fine_tuned_model = openai.FineTune.retrieve(id=fine_tune_response.id).fine_tuned_model\n",
    "\n",
    "# 8. Test the fine-tuned model\n",
    "new_prompt = \"NEW ENVIRONMENTAL CLAIM ->\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=50,\n",
    "  temperature=0.5\n",
    ")\n",
    "print(answer['choices'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p pandas,numpy,sklearn,datasets,spacy,wordcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
